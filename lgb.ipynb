{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'\n",
    "newfeature_path = './feature_test_2_ori'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "\n",
    "del train['dt']\n",
    "logging.info(\"train %s\", train.shape)\n",
    "\n",
    "test = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt']\n",
    "sub = test.copy()\n",
    "sub_size = len(sub)\n",
    "\n",
    "del test['dt']\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(train)\n",
    "len_test = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid feature\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/train1_qid_feature.pkl')\n",
    "t1 = t1.reset_index(drop= True)\n",
    "train = train.reset_index(drop= True)\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)\n",
    "\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/test2_qid_feature.pkl')\n",
    "t1 = t1.reset_index(drop= True)\n",
    "test = test.reset_index(drop= True)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载 history \n",
    "t1 = pickle.load(open(f'{feature_path}/history_feature.pkl', 'rb'))\n",
    "\n",
    "train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = \n",
    "test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 history 1\n",
    "t1 = pickle.load(open(f'{feature_path}/history_feature1.pkl', 'rb'))\n",
    "\n",
    "train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = \n",
    "test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 history 2\n",
    "t1 = pickle.load(open(f'{feature_path}/history_feature2.pkl', 'rb'))\n",
    "\n",
    "train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = \n",
    "test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载 ans kfold feature\n",
    "all_col = ['day', 'hour', 'q_inv_kfold_mean', 'q_inv_kfold_sum', 'q_inv_kfold_std', 'q_inv_kfold_count', \n",
    "           'u_inv_kfold_mean', 'u_inv_kfold_sum', 'u_inv_kfold_std', 'u_inv_kfold_count', 'q_ans_kfold_count',\n",
    "           'u_ans_kfold_count', 'q_is_good_sum', 'q_is_good_max', 'q_is_good_mean', 'u_is_good_sum',\n",
    "           'u_is_good_max', 'u_is_good_mean', 'q_is_rec_sum', 'q_is_rec_max', 'q_is_rec_mean', 'u_is_rec_sum',\n",
    "           'u_is_rec_max', 'u_is_rec_mean', 'q_is_dest_sum', 'q_is_dest_max', 'q_is_dest_mean', \n",
    "           'u_is_dest_sum', 'u_is_dest_max', 'u_is_dest_mean', 'q_has_img_sum', 'q_has_img_max', \n",
    "           'q_has_img_mean', 'u_has_img_sum', 'u_has_img_max', 'u_has_img_mean', 'q_has_video_sum', \n",
    "           'q_has_video_max', 'q_has_video_mean', 'u_has_video_sum', 'u_has_video_max', 'u_has_video_mean',\n",
    "           'q_word_count_sum', 'q_word_count_max', 'q_word_count_mean', 'u_word_count_sum', 'u_word_count_max',\n",
    "           'u_word_count_mean', 'q_reci_cheer_sum', 'q_reci_cheer_max', 'q_reci_cheer_mean', 'u_reci_cheer_sum',\n",
    "           'u_reci_cheer_max', 'u_reci_cheer_mean', 'q_reci_uncheer_sum', 'q_reci_uncheer_max', \n",
    "           'q_reci_uncheer_mean', 'u_reci_uncheer_sum', 'u_reci_uncheer_max', 'u_reci_uncheer_mean', \n",
    "           'q_reci_comment_sum', 'q_reci_comment_max', 'q_reci_comment_mean', 'u_reci_comment_sum', \n",
    "           'u_reci_comment_max', 'u_reci_comment_mean', 'q_reci_mark_sum', 'q_reci_mark_max', \n",
    "           'q_reci_mark_mean', 'u_reci_mark_sum', 'u_reci_mark_max', 'u_reci_mark_mean', 'q_reci_tks_sum',\n",
    "           'q_reci_tks_max', 'q_reci_tks_mean', 'u_reci_tks_sum', 'u_reci_tks_max', 'u_reci_tks_mean',\n",
    "           'q_reci_xxx_sum', 'q_reci_xxx_max', 'q_reci_xxx_mean', 'u_reci_xxx_sum', 'u_reci_xxx_max', \n",
    "           'u_reci_xxx_mean', 'q_reci_no_help_sum', 'q_reci_no_help_max', 'q_reci_no_help_mean', \n",
    "           'u_reci_no_help_sum', 'u_reci_no_help_max', 'u_reci_no_help_mean', 'q_reci_dis_sum', \n",
    "           'q_reci_dis_max', 'q_reci_dis_mean', 'u_reci_dis_sum', 'u_reci_dis_max', 'u_reci_dis_mean', \n",
    "           'q_diff_qa_days_sum', 'q_diff_qa_days_max', 'q_diff_qa_days_mean', 'u_diff_qa_days_sum', \n",
    "           'u_diff_qa_days_max', 'u_diff_qa_days_mean']\n",
    "drop_col = ['u_is_rec_mean', 'u_reci_uncheer_mean', 'q_is_dest_sum', 'u_reci_uncheer_sum', 'u_is_rec_max', \n",
    "             'u_is_dest_mean','q_reci_uncheer_mean', 'q_reci_uncheer_sum', 'u_is_dest_sum', 'q_is_dest_max',\n",
    "             'q_reci_uncheer_max', 'u_reci_tks_max', 'q_reci_mark_max','u_reci_dis_max', 'q_has_video_mean',\n",
    "             'q_reci_no_help_mean', 'count_u_topic', 'u_has_video_mean', 'q_reci_dis_sum', 'q_reci_mark_sum',\n",
    "             'q_reci_tks_sum','q_reci_tks_max','q_reci_dis_max','u_reci_mark_max','q_is_good_mean',\n",
    "             'q_reci_no_help_sum', 'q_reci_xxx_max', 'u_reci_xxx_max','u_reci_no_help_sum','u_reci_xxx_sum',\n",
    "              'u_is_good_mean','q_reci_no_help_max','u_has_img_max','u_is_good_sum','u_reci_no_help_max',\n",
    "              'u_has_video_sum','uf_b5','q_reci_xxx_sum','q_is_good_sum','q_has_img_max','q_has_video_sum',\n",
    "              'q_has_video_max','u_has_video_max','q_is_good_max','q_is_rec_max','u_is_good_max',\n",
    "              'q_is_dest_mean','u_reci_uncheer_max','uf_c5_count','u_is_dest_max','q_is_rec_mean',\n",
    "              'q_is_rec_sum','u_is_rec_sum', 'q_reci_xxx_mean','u_reci_xxx_mean','u_reci_comment_max',\n",
    "              'q_reci_comment_sum','u_reci_cheer_max','u_reci_dis_sum','u_reci_tks_sum','q_has_img_sum',\n",
    "              'q_reci_comment_max','q_reci_cheer_max','u_reci_no_help_mean','u_has_img_sum','u_reci_mark_sum']\n",
    "use_col = list(set(all_col) - set(drop_col))\n",
    "\n",
    "t1 = pd.read_csv(f'{feature_path}/train_kfold_feature.txt', sep='\\t', usecols=use_col)\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)\n",
    "\n",
    "\n",
    "t1 = pd.read_csv(f'{newfeature_path}/test2_kfold_feature.txt', sep='\\t', usecols=use_col)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 重命名\n",
    "t1 = []\n",
    "n_dup = 0\n",
    "for c in train.columns:\n",
    "    if c not in t1:\n",
    "        t1.append(c)\n",
    "    else:\n",
    "        t1.append(c+'_2')\n",
    "        print(c, c+'_2')\n",
    "        n_dup += 1\n",
    "print('dup num: ', n_dup)\n",
    "print('=================')\n",
    "\n",
    "print(len(t1))\n",
    "\n",
    "train.columns = t1\n",
    "\n",
    "del train['day_2']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 重命名\n",
    "t2 = []\n",
    "n_dup = 0\n",
    "for c in test.columns:\n",
    "    if c not in t2:\n",
    "        t2.append(c)\n",
    "    else:\n",
    "        t2.append(c+'_2')\n",
    "        print(c, c+'_2')\n",
    "        n_dup += 1\n",
    "print('dup num: ', n_dup)\n",
    "\n",
    "print(len(t2))\n",
    "test.columns = t2\n",
    "del test['day_2']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['week'] = train['day']%7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['week'] = test['day']%7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 invete feature 1\n",
    "t1 = pd.read_csv(f'{newfeature_path}/train2_invite_feature.txt', sep='\\t')\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(f'{newfeature_path}/test2_invite_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 invete feature 2\n",
    "t1 = pd.read_csv(f'{newfeature_path}/train2_invite_feature_2.txt', sep='\\t')\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(f'{newfeature_path}/test2_invite_feature_2.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 kfold topic feature, QU\n",
    "t1 = pd.read_csv(f'{feature_path}/train_kfold_topic_feature.txt', sep='\\t')\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(f'{newfeature_path}/newtest_kfold_topic_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 user kfold topic feature，UU\n",
    "t1 = pd.read_csv(f'{feature_path}/train_kfold_ut_feature.txt', sep='\\t')\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "logging.info(\"train %s\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(f'{newfeature_path}/newtest_kfold_ut_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "t = train.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    train[x] = train[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    train[x] = train[x].astype('float32')\n",
    "\n",
    "pickle.dump(train, open(f'{feature_path}/train_373.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = test.dtypes\n",
    "# for x in t[t == 'int64'].index:\n",
    "#     test[x] = test[x].astype('int32')\n",
    "# for x in t[t == 'float64'].index:\n",
    "#     test[x] = test[x].astype('float32')\n",
    "    \n",
    "pickle.dump(test, open(f'{feature_path}/test_373_fake.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(f'{feature_path}/train_373.pkl', 'rb'))\n",
    "test = pickle.load(open(f'{feature_path}/test_373.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载用户\n",
    "# user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "# user.columns = ['uid', 'gender', 'freq', 'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "#                 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5',  'score', 'follow_topic', 'inter_topic']\n",
    "\n",
    "# del user['follow_topic'], user['inter_topic']\n",
    "# logging.info(\"user %s\", user.shape)\n",
    "\n",
    "# unq = user.nunique()\n",
    "# logging.info(\"user unq %s\", unq)\n",
    "\n",
    "# for x in unq[unq == 1].index:\n",
    "#     del user[x]\n",
    "#     logging.info('del unq==1 %s', x)\n",
    "\n",
    "# t = user.dtypes\n",
    "# cats = [x for x in t[t == 'object'].index if x not in ['follow_topic', 'inter_topic', 'uid']]\n",
    "# logging.info(\"user cat %s\", cats)\n",
    "\n",
    "# for d in cats:\n",
    "#     lb = LabelEncoder()\n",
    "#     user[d] = lb.fit_transform(user[d])\n",
    "#     logging.info('encode %s', d)\n",
    "    \n",
    "# logging.info('encoding qid...')    \n",
    "# q_lb = LabelEncoder()\n",
    "# q_lb.fit(list(train['qid'].astype(str).values) + list(test['qid'].astype(str).values))\n",
    "# train['qid_enc'] = q_lb.transform(train['qid'])\n",
    "# test['qid_enc'] = q_lb.transform(test['qid'])\n",
    "# logging.info('add qid_enc')\n",
    "\n",
    "# logging.info('encoding uid...')\n",
    "# u_lb = LabelEncoder()\n",
    "# u_lb.fit(user['uid'])\n",
    "# train['uid_enc'] = u_lb.transform(train['uid'])\n",
    "# test['uid_enc'] = u_lb.transform(test['uid'])\n",
    "# logging.info('add uid_enc')\n",
    "\n",
    "# # merge user\n",
    "# train = pd.merge(train, user, on='uid', how='left')\n",
    "# test = pd.merge(test, user, on='uid', how='left')\n",
    "# logging.info(\"train shape %s, test shape %s\", train.shape, test.shape)\n",
    "\n",
    "# del user\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
=======
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'freq', 'uf_b1', 'uf_b2', 'uf_b3', 'uf_b4', 'uf_b5', 'uf_c1',\n",
      "       'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', 'score', 'qid_enc', 'uid_enc'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 04:30:44,129] INFO in <ipython-input-5-53513dccab8c>: train shape: (9489162, 388)\n",
      "[2019-12-16 04:30:52,330] INFO in <ipython-input-5-53513dccab8c>: test shape: (1141683, 387)\n"
     ]
    }
   ],
   "source": [
    "# 用户问题等信息 encoding\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/train_member_basic_feature.pkl')\n",
    "# print(t1.keys())\n",
    "# t1 = t1.reset_index(drop= True)\n",
    "# train = train[[f for f in train.keys() if f not in t1.keys()]]\n",
    "# train = train.reset_index(drop= True)\n",
    "train = pd.concat([train, t1], axis=1)\n",
<<<<<<< HEAD
    "logging.info('train shape: %s', train.shape)\n",
    "\n",
    "t1 = pd.read_pickle(f'{feature_path}/test_member_basic_feature.pkl')\n",
    "t1 = t1.reset_index(drop= True)\n",
    "test = test[[f for f in test.keys() if f not in t1.keys()]]\n",
    "test = test.reset_index(drop= True)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
=======
    "logging.info('train shape: %s', train.shape)"
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat((train, test), axis=0, sort=True)\n",
    "len_train = len(train)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_pickle(f'{newfeature_path}/test_member_basic_feature.pkl')\n",
    "# t1 = t1.reset_index(drop= True)\n",
    "# test = test[[f for f in test.keys() if f not in t1.keys()]]\n",
    "# test = test.reset_index(drop= True)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 04:43:52,256] INFO in <ipython-input-7-69ba8a19e379>: t1 shape: (10630845, 233)\n",
      "[2019-12-16 05:14:41,802] INFO in <ipython-input-7-69ba8a19e379>: data shape: (10630845, 621)\n"
     ]
    }
   ],
   "source": [
    "t1 = pd.read_csv(f'{feature_path}/count_features.txt', sep='\\t')\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "\n",
    "data = data[[f for f in data.keys() if f not in t1.keys()]]\n",
    "t1 = t1.reset_index(drop= True)\n",
    "data = pd.concat([data.reset_index(drop=True), t1], axis=1)\n",
    "logging.info('data shape: %s', data.shape)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((train, test), axis=0, sort=True)\n",
    "logging.info('data shape: %s', data.shape)\n",
    "len_train = len(train)\n",
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')"
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# t1 = pickle.load(open(f'{feature_path}/history_lastweek_sup_a.pkl', 'rb'))\n",
    "# data = pd.concat([data, t1], axis=1)\n",
    "# logging.info('data shape %s', data.shape)\n",
    "\n",
    "# t1 = pickle.load(open(f'{feature_path}/history_ltd6_sup_a.pkl', 'rb'))\n",
    "# data = pd.concat([data, t1], axis=1)\n",
    "# logging.info('data shape %s', data.shape)\n",
    "\n",
    "# t1 = pickle.load(open(f'{feature_path}/history_ltd_sup_a.pkl', 'rb'))\n",
    "# data = pd.concat([data, t1], axis=1)\n",
    "# logging.info('data shape %s', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del t1\n",
=======
    "del train, test\n",
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = pd.read_csv(f'{newfeature_path}/count_features.txt', sep='\\t')\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/count_features.pkl')\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "\n",
    "# data = data[[f for f in data.keys() if f not in t1.keys()]]\n",
    "# t1 = t1.reset_index(drop= True)\n",
    "data = pd.concat([data.reset_index(drop=True), t1], axis=1)\n",
    "logging.info('data shape: %s', data.shape)\n",
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 05:14:44,980] INFO in <ipython-input-9-26f8bd701c43>: sub (1141683, 3)\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "sub.columns = ['qid', 'uid', 'dt']\n",
    "sub_size = len(sub)\n",
    "\n",
    "logging.info(\"sub %s\", sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feat = ['label', 'uid', 'qid', 'dt']\n",
    "drop_feat += ['gender_count','qid_enc_day_diff_iq_day_mean','qid_enc_day_diff_iq_day_median','uid_enc_day_score_mean',\n",
    "              'uid_enc_day_score_median','uf_b1_day_diff_iq_hour_mean','uf_b2_day_diff_iq_hour_mean','uf_b3_day_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_day_mean','qid_enc_day_hour_diff_iq_day_median','qid_enc_day_hour_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_hour_median','uid_enc_day_hour_score_mean','uid_enc_day_hour_score_median',\n",
    "              'uid_enc_qid_enc_score_mean','uid_enc_qid_enc_score_median','uid_enc_wk_score_mean','uid_enc_wk_score_median',\n",
    "              'freq_wk_score_median','freq_wk_diff_iq_hour_mean','gender_wk_diff_iq_day_median','gender_wk_diff_iq_hour_mean',\n",
    "              'uf_b1_wk_diff_iq_day_median','uf_b1_wk_diff_iq_hour_mean','uf_b2_wk_score_median','uf_b2_wk_diff_iq_day_median',\n",
    "              'uf_b2_wk_diff_iq_hour_mean','uf_b3_wk_diff_iq_hour_mean','uf_b4_wk_score_median','uf_b4_wk_diff_iq_day_median',\n",
    "              'uf_b4_wk_diff_iq_hour_mean','uf_b5_wk_diff_iq_day_median','uf_b5_wk_diff_iq_hour_mean','uf_c5_wk_diff_iq_hour_mean']\n",
<<<<<<< HEAD
    "drop_feat += ['uf_b4_min','dayuf_b4_min','dayuf_b4_median','uf_c5_count','dayuf_b3_min','dayuf_b3_median',\n",
    "              'qid_wk_uf_b4_median','dayuf_b5_median','dayuf_b2_median','uf_b5_median','dayuf_b1_min',\n",
    "              'uf_b5_min','wk','uf_c5_min','dayuf_b2_min','dayuf_b5_min','qid_wk_uf_b5_median','uf_b3_median',\n",
    "              'qid_enc_day_score_mean','dayuf_c5_min','qid_enc_day_score_median','uf_b2_min','uf_b1_min',\n",
    "              'qid_wk_uf_b1_min','qid_wk_uf_c5_min','qid_wk_uf_b2_median','qid_wk_uf_b2_min',\n",
    "              'qid_wk_uf_b3_median','qid_wk_uf_b3_min','qid_wk_uf_b5_min','qid_wk_uf_b4_min']\n",
    "\n",
    "# drop_feat += ['u_is_rec_mean', 'u_reci_uncheer_mean', 'q_is_dest_sum', 'u_reci_uncheer_sum', 'u_is_rec_max', \n",
    "#              'u_is_dest_mean','q_reci_uncheer_mean', 'q_reci_uncheer_sum', 'u_is_dest_sum', 'q_is_dest_max',\n",
    "#              'q_reci_uncheer_max', 'u_reci_tks_max', 'q_reci_mark_max','u_reci_dis_max', 'q_has_video_mean',\n",
    "#              'q_reci_no_help_mean', 'count_u_topic', 'u_has_video_mean', 'q_reci_dis_sum', 'q_reci_mark_sum',\n",
    "#              'q_reci_tks_sum','q_reci_tks_max','q_reci_dis_max','u_reci_mark_max','q_is_good_mean',\n",
    "#              'q_reci_no_help_sum', 'q_reci_xxx_max', 'u_reci_xxx_max','u_reci_no_help_sum','u_reci_xxx_sum',\n",
    "#               'u_is_good_mean','q_reci_no_help_max','u_has_img_max','u_is_good_sum','u_reci_no_help_max',\n",
    "#               'u_has_video_sum','uf_b5','q_reci_xxx_sum','q_is_good_sum','q_has_img_max','q_has_video_sum',\n",
    "#               'q_has_video_max','u_has_video_max','q_is_good_max','q_is_rec_max','u_is_good_max',\n",
    "#               'q_is_dest_mean','u_reci_uncheer_max','uf_c5_count','u_is_dest_max','q_is_rec_mean',\n",
    "#               'q_is_rec_sum','u_is_rec_sum', 'q_reci_xxx_mean','u_reci_xxx_mean','u_reci_comment_max',\n",
    "#               'q_reci_comment_sum','u_reci_cheer_max','u_reci_dis_sum','u_reci_tks_sum','q_has_img_sum',\n",
    "#               'q_reci_comment_max','q_reci_cheer_max','u_reci_no_help_mean','u_has_img_sum','u_reci_mark_sum']\n",
    "\n",
=======
    "drop_feat += ['uf_b2_max','uf_c5_max','dayuf_b2_max','qid_wk_uf_b3_median','qid_wk_uf_b1_max','dayuf_b1_max','qid_wk_uf_b2_median',\n",
    "              'dayuf_b1_min','dayuf_c5_min','uf_b2_median','qid_wk_uf_b4_min','dayuf_b4_min','uf_b5_min','uf_c5_min','qid_wk_uf_b1_min',\n",
    "              'qid_wk_uf_b5_max','uf_b1_max','dayuf_b3_max','dayuf_b2_min','uf_b4_max','uf_b3','uf_b5_max','qid_wk_uf_c5_median','uf_b4_min',\n",
    "              'uf_b2_min','dayuf_b5_min','qid_wk_uf_b2_min','qid_wk_uf_b4_median','qid_wk_uf_b3_max','qid_enc_day_score_mean',\n",
    "              'qid_wk_uf_c5_min','dayuf_b3_median','uf_b5_median','uf_b3_min','dayuf_b5_max','qid_wk_uf_b5_min','qid_wk_uf_b2_max',\n",
    "              'dayuf_b3_min','dayuf_b4_max','qid_enc_day_score_median','qid_wk_uf_b1_median','dayuf_b2_median','dayuf_c5_max',\n",
    "              'dayuf_b5_median','dayuf_c5_median']\n",
    "\n",
    "cate_feats = ['qid_enc', 'uid_enc', 'freq', 'gender', 'uf_b1', 'uf_b2', 'uf_b3', 'uf_b4', 'uf_b5', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5']\n",
    "val_feats = ['score', 'diff_iq_day', 'diff_iq_hour']\n",
    "for fi in cate_feats:\n",
    "    for fj in val_feats:\n",
    "        sub_feats = ['mean', 'median']\n",
    "        for sub in sub_feats:\n",
    "            if fi+'_day_hour_'+fj+'_'+sub not in drop_feat:\n",
    "                drop_feat.append(fi+'_day_hour_'+fj+'_'+sub)\n",
    "\n",
    "last_100_feat = pickle.load(open('./last100col.pkl', 'rb'))\n",
    "print(len(last_100_feat))\n",
    "for f in last_100_feat:\n",
    "    if f not in drop_feat:\n",
    "        drop_feat.append(f)\n",
    "            \n",
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
    "feature_with_day = [x for x in data.columns if x not in drop_feat]\n",
    "feature_cols = [x for x in data.columns if x not in drop_feat+['day']]\n",
    "print(len(feature_with_day))\n",
    "print(len(set(feature_with_day)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_u_topic</th>\n",
       "      <th>day</th>\n",
       "      <th>dayscore_max</th>\n",
       "      <th>dayscore_mean</th>\n",
       "      <th>dayscore_median</th>\n",
       "      <th>dayscore_min</th>\n",
       "      <th>dayscore_std</th>\n",
       "      <th>dayuf_b1_max</th>\n",
       "      <th>dayuf_b1_mean</th>\n",
       "      <th>dayuf_b1_median</th>\n",
       "      <th>...</th>\n",
       "      <th>uf_c4_day_hour_diff_iq_day_mean</th>\n",
       "      <th>uf_c4_day_hour_diff_iq_day_median</th>\n",
       "      <th>uf_c4_day_hour_diff_iq_hour_mean</th>\n",
       "      <th>uf_c4_day_hour_diff_iq_hour_median</th>\n",
       "      <th>uf_c5_day_hour_score_mean</th>\n",
       "      <th>uf_c5_day_hour_score_median</th>\n",
       "      <th>uf_c5_day_hour_diff_iq_day_mean</th>\n",
       "      <th>uf_c5_day_hour_diff_iq_day_median</th>\n",
       "      <th>uf_c5_day_hour_diff_iq_hour_mean</th>\n",
       "      <th>uf_c5_day_hour_diff_iq_hour_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3</td>\n",
       "      <td>3864</td>\n",
       "      <td>486</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.0</td>\n",
       "      <td>486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.761818</td>\n",
       "      <td>3.0</td>\n",
       "      <td>709.34980</td>\n",
       "      <td>77.0</td>\n",
       "      <td>484.16827</td>\n",
       "      <td>481.0</td>\n",
       "      <td>33.357883</td>\n",
       "      <td>4.0</td>\n",
       "      <td>795.44696</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411050</th>\n",
       "      <td>20</td>\n",
       "      <td>3860</td>\n",
       "      <td>602</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>544.0</td>\n",
       "      <td>442</td>\n",
       "      <td>73.769913</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>17.306250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.20624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>432.45690</td>\n",
       "      <td>401.0</td>\n",
       "      <td>24.370571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>588.01874</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994748</th>\n",
       "      <td>7</td>\n",
       "      <td>3871</td>\n",
       "      <td>834</td>\n",
       "      <td>517.933350</td>\n",
       "      <td>483.5</td>\n",
       "      <td>342</td>\n",
       "      <td>121.400429</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.485394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.01068</td>\n",
       "      <td>8.0</td>\n",
       "      <td>467.62726</td>\n",
       "      <td>452.0</td>\n",
       "      <td>11.606268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.73834</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000000</th>\n",
       "      <td>0</td>\n",
       "      <td>3872</td>\n",
       "      <td>346</td>\n",
       "      <td>322.666656</td>\n",
       "      <td>311.0</td>\n",
       "      <td>311</td>\n",
       "      <td>20.207260</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86.07407</td>\n",
       "      <td>75.0</td>\n",
       "      <td>379.43930</td>\n",
       "      <td>356.0</td>\n",
       "      <td>14.244305</td>\n",
       "      <td>3.0</td>\n",
       "      <td>346.42944</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 621 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          count_u_topic   day  dayscore_max  dayscore_mean  dayscore_median  \\\n",
       "100                   3  3864           486     486.000000            486.0   \n",
       "9411050              20  3860           602     533.000000            544.0   \n",
       "9994748               7  3871           834     517.933350            483.5   \n",
       "10000000              0  3872           346     322.666656            311.0   \n",
       "\n",
       "          dayscore_min  dayscore_std  dayuf_b1_max  dayuf_b1_mean  \\\n",
       "100                486           NaN             0       0.000000   \n",
       "9411050            442     73.769913             1       0.500000   \n",
       "9994748            342    121.400429             1       0.633333   \n",
       "10000000           311     20.207260             1       1.000000   \n",
       "\n",
       "          dayuf_b1_median  ...  uf_c4_day_hour_diff_iq_day_mean  \\\n",
       "100                   0.0  ...                        29.761818   \n",
       "9411050               0.5  ...                        17.306250   \n",
       "9994748               1.0  ...                         9.485394   \n",
       "10000000              1.0  ...                         3.444444   \n",
       "\n",
       "          uf_c4_day_hour_diff_iq_day_median  uf_c4_day_hour_diff_iq_hour_mean  \\\n",
       "100                                     3.0                         709.34980   \n",
       "9411050                                 0.0                         418.20624   \n",
       "9994748                                 0.0                         226.01068   \n",
       "10000000                                3.0                          86.07407   \n",
       "\n",
       "          uf_c4_day_hour_diff_iq_hour_median  uf_c5_day_hour_score_mean  \\\n",
       "100                                     77.0                  484.16827   \n",
       "9411050                                  3.0                  432.45690   \n",
       "9994748                                  8.0                  467.62726   \n",
       "10000000                                75.0                  379.43930   \n",
       "\n",
       "          uf_c5_day_hour_score_median  uf_c5_day_hour_diff_iq_day_mean  \\\n",
       "100                             481.0                        33.357883   \n",
       "9411050                         401.0                        24.370571   \n",
       "9994748                         452.0                        11.606268   \n",
       "10000000                        356.0                        14.244305   \n",
       "\n",
       "          uf_c5_day_hour_diff_iq_day_median  uf_c5_day_hour_diff_iq_hour_mean  \\\n",
       "100                                     4.0                         795.44696   \n",
       "9411050                                 0.0                         588.01874   \n",
       "9994748                                 0.0                         276.73834   \n",
       "10000000                                3.0                         346.42944   \n",
       "\n",
       "          uf_c5_day_hour_diff_iq_hour_median  \n",
       "100                                     85.0  \n",
       "9411050                                  3.0  \n",
       "9994748                                  8.0  \n",
       "10000000                                83.0  \n",
       "\n",
       "[4 rows x 621 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[100,9411050,9994748, 10000000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 05:16:36,452] INFO in <ipython-input-12-118e89832e3c>: feature size 572\n",
      "[2019-12-16 05:16:48,867] INFO in <ipython-input-12-118e89832e3c>: X_train_all shape: (9489162, 573)\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"feature size %s\", len(feature_cols))\n",
    "\n",
    "# X_train_all = data.iloc[:len_train][feature_with_day]\n",
    "# y_train_all = data.iloc[:len_train]['label']\n",
    "\n",
    "X_train_all = data[:len_train][feature_with_day]\n",
    "y_train_all = data[:len_train]['label']\n",
    "X_test = data[len_train:]\n",
    "assert len(X_test) == sub_size\n",
    "logging.info('X_train_all shape: %s', X_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 05:27:08,465] INFO in <ipython-input-15-607bbfac2ecc>: get index.\n",
      "[2019-12-16 05:27:08,549] INFO in <ipython-input-15-607bbfac2ecc>: start to split data\n",
      "[2019-12-16 09:59:42,466] INFO in <ipython-input-15-607bbfac2ecc>: train shape (9141216, 572), val shape (347946, 572), test shape (1141683, 621)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# del data\n",
    "# gc.collect()\n",
    "\n",
    "# fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for index, (train_idx, val_idx) in enumerate(fold.split(X=X_train_all, y=y_train_all)):\n",
    "#     break\n",
    "\n",
    "# X_train, X_val, y_train, y_val = X_train_all.iloc[train_idx][feature_cols], X_train_all.iloc[val_idx][feature_cols], \\\n",
    "#                                  y_train_all.iloc[train_idx], \\\n",
    "#                                  y_train_all.iloc[val_idx]\n",
    "# del X_train_all\n",
    "# gc.collect()\n",
    "\n",
    "logging.info('get index.')\n",
    "index_1 = pickle.load(open(f'./train_index.pkl', 'rb'))\n",
    "index_2 = pickle.load(open(f'./val_index.pkl', 'rb'))\n",
    "\n",
    "logging.info('start to split data')\n",
    "X_val = X_train_all.loc[index_2][feature_cols]\n",
    "X_train = X_train_all.loc[index_1][feature_cols]\n",
    "\n",
    "y_val = y_train_all.loc[index_2]\n",
    "y_train = y_train_all.loc[index_1]\n",
    "\n",
    "logging.info(\"train shape %s, val shape %s, test shape %s\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train_all, y_train_all, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_854 = pickle.load(open('./0.854cols.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in col_854:\n",
    "#     if c not in not_his_col:\n",
    "#         print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_his_col = ['invite_hour', 'invite_day', 'ffa', 'sex', 'invite_day_count_y_median_x', 'invite_day_count_x_count_y', \n",
    "               'invite_day_count_y_std_y', 'invite_hour_std_y', 'invite_day_count_x_mean_x', 'invite_create', 'invite_day_count_x_mean_y', \n",
    "               'invite_hour_mean_y', 'invite_day_count_x_count_x', 'fa', 'salt_value', 'ffe', 'cross_topic', 'invite_day_count_y_count_x', \n",
    "               'sex_count', 'fd', 'invite_hour_std_x', 'invite_day_count_y', 'invite_day_count_y_median_y', 'create_hour', \n",
    "               'invite_hour_mean_x', 'fb', 'invite_day_count_x_median_y', 'fe', 'ffd', 'invite_day_count_x_std_x', 'ffc',\n",
    "               'invite_day_count_y_mean_x', 'invite_day_count_x_std_y', 'invite_day_count_y_std_x', 'ffb', 'invite_day_count_x_median_x', \n",
    "               'invite_day_count_y_count_y', 'invite_day_count_x', 'create_day', 'fc', 'invite_day_count_y_mean_y']\n",
    "for c1 in col_854:\n",
    "    assert isinstance(c1, str)\n",
    "    if (c1 not in feature_with_day) and (c1 not in not_his_col): \n",
    "        feature_with_day.append(c1)\n",
    "        feature_cols.append(c1)\n",
    "print(len(feature_with_day))\n",
    "print(len(feature_cols))\n",
    "print(len(set(feature_with_day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pickle.load(open(f'{feature_path}/history_lastweek_sup_a.pkl', 'rb'))\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('data shape %s', data.shape)\n",
    "\n",
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')\n",
    "\n",
    "t1 = pickle.load(open(f'{feature_path}/history_ltd6_sup_a.pkl', 'rb'))\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('data shape %s', data.shape)\n",
    "\n",
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')\n",
    "\n",
    "t1 = pickle.load(open(f'{feature_path}/history_ltd_sup_a.pkl', 'rb'))\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('data shape %s', data.shape)\n",
    "\n",
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "# sub.columns = ['qid', 'uid', 'dt']\n",
    "# sub_size = len(sub)\n",
    "\n",
    "# logging.info(\"sub %s\", sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"feature size %s\", len(feature_cols))\n",
    "\n",
    "# X_train_all = data.iloc[:len_train][feature_with_day]\n",
    "# y_train_all = data.iloc[:len_train]['label']\n",
    "\n",
    "X_train_all = data[:len_train][feature_with_day]\n",
    "y_train_all = data[:len_train]['label']\n",
    "X_test = data[len_train:]\n",
    "# assert len(X_test) == sub_size\n",
    "logging.info('X_train_all shape: %s', X_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('X_train_all shape: %s', X_train_all.shape)"
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "# pickle.dump(index_1, open(f'./train_index.pkl', 'wb'))\n",
    "# pickle.dump(index_2, open(f'./val_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "val_set = lgb.Dataset(X_val, y_val, free_raw_data=False)\n",
    "del X_val, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', #xentlambda\n",
    "    'metric': 'auc',\n",
    "    'silent':0,\n",
    "    'learning_rate': 0.01,\n",
    "    'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "    'num_leaves': 512,  # we should let it be smaller than 2^(max_depth)\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 10,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 425,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.5,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 1,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    #'scale_pos_weight':100,\n",
    "    'subsample_for_bin': 50000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 2.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 5,  # L2 regularization term on weights\n",
    "#     'nthread': 10,\n",
    "    'verbose': 0,\n",
    "}"
=======
   "source": [
    "pickle.dump(X_test, open('./X_test_in_train.pkl', 'wb'))\n",
    "pickle.dump(feature_with_day, open('./feature_with_day_in_train.pkl', 'wb'))\n",
    "pickle.dump(feature_cols, open('./feature_cols_in_train.pkl', 'wb'))"
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "final_round = 4000\n",
    "save_round = 1000\n",
    "current_round = 0\n",
    "\n",
    "model_lgb = None\n",
    "while current_round<final_round:\n",
    "    logging.info('start training, round: %s, ', current_round)\n",
    "    model_lgb = lgb.train(params, train_set=train_set, valid_sets=val_set, \n",
    "                          num_boost_round=save_round, early_stopping_rounds=50,\n",
    "                          init_model=model_lgb,\n",
    "                          keep_training_booster=True)\n",
    "    current_round += save_round\n",
    "    pickle.dump(model_lgb, open(f'./model/lgb_{current_round}_round.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb = LGBMClassifier(n_estimators=2000, num_leaves=512, n_jobs=-1, objective='binary', learning_rate=0.01,\n",
    "#                            seed=1000, silent=True, max_bin=425, subsample_for_bin=50000, min_split_gain=0,max_depth=-1,\n",
    "#                           min_child_weight=1, min_child_samples=10, subsample=0.8, subsample_freq=1, is_unbalance=True, \n",
    "#                           colsample_bytree=0.5, reg_alpha=2.99, reg_lambda=5)\n",
    "# # model_lgb = LGBMClassifier(n_estimators=2000, n_jobs=-1, objective='binary', seed=1000, silent=True)\n",
    "# model_lgb.fit(X_train, y_train,         \n",
    "#               eval_metric=['auc'],\n",
    "#               eval_set=[(X_val, y_val)],\n",
    "#               early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('get index.')\n",
    "index_1 = pickle.load(open(f'./train_index.pkl', 'rb'))\n",
    "index_2 = pickle.load(open(f'./val_index.pkl', 'rb'))\n",
    "\n",
    "logging.info('start to split data')\n",
    "X_val = X_train_all.loc[index_2][feature_cols]\n",
    "X_train = X_train_all.loc[index_1][feature_cols]\n",
    "\n",
    "y_val = y_train_all.loc[index_2]\n",
    "y_train = y_train_all.loc[index_1]\n",
    "\n",
    "logging.info(\"train shape %s, val shape %s, test shape %s\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_all, y_train_all, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(index_1, open(f'./train_index.pkl', 'wb'))\n",
    "# pickle.dump(index_2, open(f'./val_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "val_set = lgb.Dataset(X_val, y_val, free_raw_data=False)\n",
    "del X_val, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', #xentlambda\n",
    "    'metric': 'auc',\n",
    "    'silent':0,\n",
    "    'learning_rate': 0.01,\n",
    "    'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "    'num_leaves': 512,  # we should let it be smaller than 2^(max_depth)\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 10,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 425,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.5,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 1,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    #'scale_pos_weight':100,\n",
    "    'subsample_for_bin': 50000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 2.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 3,  # L2 regularization term on weights\n",
    "#     'nthread': 10,\n",
    "    'verbose': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_round = 5000\n",
    "save_round = 500\n",
    "current_round = 0\n",
    "\n",
    "model_lgb = None\n",
    "while current_round<final_round:\n",
    "    logging.info('start training, round: %s, ', current_round)\n",
    "    model_lgb = lgb.train(params, train_set=train_set, valid_sets=val_set, \n",
    "                          num_boost_round=save_round, early_stopping_rounds=50,\n",
    "                          init_model=model_lgb,\n",
    "                          keep_training_booster=True)\n",
    "    current_round += save_round\n",
    "    pickle.dump(model_lgb, open(f'./model/lgb_label_{current_round}_round.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": true
   },
=======
   "metadata": {},
>>>>>>> eed18d6427642b71f888050a6749e6fe3bf1e86a
   "outputs": [],
   "source": [
    "del X_test, model_lgb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_round = 5000\n",
    "save_round = 500\n",
    "current_round = 2000\n",
    "\n",
    "model_lgb = pickle.load(open('./model/lgb_label_2000_round.pkl', 'rb'))\n",
    "while current_round<final_round:\n",
    "    logging.info('start training, round: %s, ', current_round)\n",
    "    model_lgb = lgb.train(params, train_set=train_set, valid_sets=val_set, \n",
    "                          num_boost_round=save_round, early_stopping_rounds=50,\n",
    "                          init_model=model_lgb,\n",
    "                          keep_training_booster=True)\n",
    "    current_round += save_round\n",
    "    pickle.dump(model_lgb, open(f'./model/lgb_label_{current_round}_round.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
