{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import logging\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:51:43,107] INFO in <ipython-input-4-c899d16ad23a>: train (9489162, 3)\n",
      "[2019-12-14 06:51:44,792] INFO in <ipython-input-4-c899d16ad23a>: test (1141683, 2)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "# train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "\n",
    "# del train['dt']\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "# test.columns = ['qid', 'uid', 'dt']\n",
    "# sub = test.copy()\n",
    "# sub_size = len(sub)\n",
    "\n",
    "# del test['dt']\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_train = len(train)\n",
    "# len_test = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:52:18,489] INFO in <ipython-input-6-277cc9743066>: train (9489162, 170)\n",
      "[2019-12-14 06:52:22,568] INFO in <ipython-input-6-277cc9743066>: test (1141683, 169)\n"
     ]
    }
   ],
   "source": [
    "# qid feature\n",
    "# t1 = pd.read_pickle(f'{feature_path}/train_qid_feature.pkl')\n",
    "# t1 = t1.reset_index(drop= True)\n",
    "# train = train.reset_index(drop= True)\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# t1 = pd.read_pickle(f'{feature_path}/test_qid_feature.pkl')\n",
    "# t1 = t1.reset_index(drop= True)\n",
    "# test = test.reset_index(drop= True)\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:52:46,162] INFO in <ipython-input-7-6525bd882acf>: train (9489162, 196)\n",
      "[2019-12-14 06:52:48,742] INFO in <ipython-input-7-6525bd882acf>: test (1141683, 195)\n"
     ]
    }
   ],
   "source": [
    "# 加载 history \n",
    "# t1 = pickle.load(open(f'{feature_path}/history_feature.pkl', 'rb'))\n",
    "\n",
    "# train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:53:15,141] INFO in <ipython-input-8-e35bf51534b0>: train (9489162, 222)\n",
      "[2019-12-14 06:53:18,117] INFO in <ipython-input-8-e35bf51534b0>: test (1141683, 221)\n"
     ]
    }
   ],
   "source": [
    "# 加载 history 1\n",
    "# t1 = pickle.load(open(f'{feature_path}/history_feature1.pkl', 'rb'))\n",
    "\n",
    "# train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:53:48,047] INFO in <ipython-input-9-2ebbf6b9bccc>: train (9489162, 248)\n",
      "[2019-12-14 06:53:51,407] INFO in <ipython-input-9-2ebbf6b9bccc>: test (1141683, 247)\n"
     ]
    }
   ],
   "source": [
    "# 加载 history 2\n",
    "# t1 = pickle.load(open(f'{feature_path}/history_feature2.pkl', 'rb'))\n",
    "\n",
    "# train = pd.concat([train, t1[:len_train]], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# test = pd.concat([test, t1[len_train:].reset_index(drop=True)], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 06:56:42,177] INFO in <ipython-input-13-df91efe901e4>: train (9489162, 287)\n",
      "[2019-12-14 06:56:52,064] INFO in <ipython-input-13-df91efe901e4>: test (1141683, 286)\n"
     ]
    }
   ],
   "source": [
    "# 加载 ans kfold feature\n",
    "# all_col = ['day', 'hour', 'q_inv_kfold_mean', 'q_inv_kfold_sum', 'q_inv_kfold_std', 'q_inv_kfold_count', \n",
    "#            'u_inv_kfold_mean', 'u_inv_kfold_sum', 'u_inv_kfold_std', 'u_inv_kfold_count', 'q_ans_kfold_count',\n",
    "#            'u_ans_kfold_count', 'q_is_good_sum', 'q_is_good_max', 'q_is_good_mean', 'u_is_good_sum',\n",
    "#            'u_is_good_max', 'u_is_good_mean', 'q_is_rec_sum', 'q_is_rec_max', 'q_is_rec_mean', 'u_is_rec_sum',\n",
    "#            'u_is_rec_max', 'u_is_rec_mean', 'q_is_dest_sum', 'q_is_dest_max', 'q_is_dest_mean', \n",
    "#            'u_is_dest_sum', 'u_is_dest_max', 'u_is_dest_mean', 'q_has_img_sum', 'q_has_img_max', \n",
    "#            'q_has_img_mean', 'u_has_img_sum', 'u_has_img_max', 'u_has_img_mean', 'q_has_video_sum', \n",
    "#            'q_has_video_max', 'q_has_video_mean', 'u_has_video_sum', 'u_has_video_max', 'u_has_video_mean',\n",
    "#            'q_word_count_sum', 'q_word_count_max', 'q_word_count_mean', 'u_word_count_sum', 'u_word_count_max',\n",
    "#            'u_word_count_mean', 'q_reci_cheer_sum', 'q_reci_cheer_max', 'q_reci_cheer_mean', 'u_reci_cheer_sum',\n",
    "#            'u_reci_cheer_max', 'u_reci_cheer_mean', 'q_reci_uncheer_sum', 'q_reci_uncheer_max', \n",
    "#            'q_reci_uncheer_mean', 'u_reci_uncheer_sum', 'u_reci_uncheer_max', 'u_reci_uncheer_mean', \n",
    "#            'q_reci_comment_sum', 'q_reci_comment_max', 'q_reci_comment_mean', 'u_reci_comment_sum', \n",
    "#            'u_reci_comment_max', 'u_reci_comment_mean', 'q_reci_mark_sum', 'q_reci_mark_max', \n",
    "#            'q_reci_mark_mean', 'u_reci_mark_sum', 'u_reci_mark_max', 'u_reci_mark_mean', 'q_reci_tks_sum',\n",
    "#            'q_reci_tks_max', 'q_reci_tks_mean', 'u_reci_tks_sum', 'u_reci_tks_max', 'u_reci_tks_mean',\n",
    "#            'q_reci_xxx_sum', 'q_reci_xxx_max', 'q_reci_xxx_mean', 'u_reci_xxx_sum', 'u_reci_xxx_max', \n",
    "#            'u_reci_xxx_mean', 'q_reci_no_help_sum', 'q_reci_no_help_max', 'q_reci_no_help_mean', \n",
    "#            'u_reci_no_help_sum', 'u_reci_no_help_max', 'u_reci_no_help_mean', 'q_reci_dis_sum', \n",
    "#            'q_reci_dis_max', 'q_reci_dis_mean', 'u_reci_dis_sum', 'u_reci_dis_max', 'u_reci_dis_mean', \n",
    "#            'q_diff_qa_days_sum', 'q_diff_qa_days_max', 'q_diff_qa_days_mean', 'u_diff_qa_days_sum', \n",
    "#            'u_diff_qa_days_max', 'u_diff_qa_days_mean']\n",
    "# drop_col = ['u_is_rec_mean', 'u_reci_uncheer_mean', 'q_is_dest_sum', 'u_reci_uncheer_sum', 'u_is_rec_max', \n",
    "#              'u_is_dest_mean','q_reci_uncheer_mean', 'q_reci_uncheer_sum', 'u_is_dest_sum', 'q_is_dest_max',\n",
    "#              'q_reci_uncheer_max', 'u_reci_tks_max', 'q_reci_mark_max','u_reci_dis_max', 'q_has_video_mean',\n",
    "#              'q_reci_no_help_mean', 'count_u_topic', 'u_has_video_mean', 'q_reci_dis_sum', 'q_reci_mark_sum',\n",
    "#              'q_reci_tks_sum','q_reci_tks_max','q_reci_dis_max','u_reci_mark_max','q_is_good_mean',\n",
    "#              'q_reci_no_help_sum', 'q_reci_xxx_max', 'u_reci_xxx_max','u_reci_no_help_sum','u_reci_xxx_sum',\n",
    "#               'u_is_good_mean','q_reci_no_help_max','u_has_img_max','u_is_good_sum','u_reci_no_help_max',\n",
    "#               'u_has_video_sum','uf_b5','q_reci_xxx_sum','q_is_good_sum','q_has_img_max','q_has_video_sum',\n",
    "#               'q_has_video_max','u_has_video_max','q_is_good_max','q_is_rec_max','u_is_good_max',\n",
    "#               'q_is_dest_mean','u_reci_uncheer_max','uf_c5_count','u_is_dest_max','q_is_rec_mean',\n",
    "#               'q_is_rec_sum','u_is_rec_sum', 'q_reci_xxx_mean','u_reci_xxx_mean','u_reci_comment_max',\n",
    "#               'q_reci_comment_sum','u_reci_cheer_max','u_reci_dis_sum','u_reci_tks_sum','q_has_img_sum',\n",
    "#               'q_reci_comment_max','q_reci_cheer_max','u_reci_no_help_mean','u_has_img_sum','u_reci_mark_sum']\n",
    "# use_col = list(set(all_col) - set(drop_col))\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/train_kfold_feature.txt', sep='\\t', usecols=use_col)\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/test_kfold_feature.txt', sep='\\t', usecols=use_col)\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup num:  0\n",
      "=================\n",
      "uid_history_inv_count uid_history_inv_count_2\n",
      "uid_history_label_mean uid_history_label_mean_2\n",
      "qid_history_inv_count qid_history_inv_count_2\n",
      "qid_history_label_mean qid_history_label_mean_2\n",
      "qid_history_uf_b1_mean qid_history_uf_b1_mean_2\n",
      "qid_history_uf_b1_std qid_history_uf_b1_std_2\n",
      "qid_history_uf_b2_mean qid_history_uf_b2_mean_2\n",
      "qid_history_uf_b2_std qid_history_uf_b2_std_2\n",
      "qid_history_uf_b3_mean qid_history_uf_b3_mean_2\n",
      "qid_history_uf_b3_std qid_history_uf_b3_std_2\n",
      "qid_history_uf_b4_mean qid_history_uf_b4_mean_2\n",
      "qid_history_uf_b4_std qid_history_uf_b4_std_2\n",
      "qid_history_uf_b5_mean qid_history_uf_b5_mean_2\n",
      "qid_history_uf_b5_std qid_history_uf_b5_std_2\n",
      "qid_history_uf_c1_mean qid_history_uf_c1_mean_2\n",
      "qid_history_uf_c1_std qid_history_uf_c1_std_2\n",
      "qid_history_uf_c2_mean qid_history_uf_c2_mean_2\n",
      "qid_history_uf_c2_std qid_history_uf_c2_std_2\n",
      "qid_history_uf_c3_mean qid_history_uf_c3_mean_2\n",
      "qid_history_uf_c3_std qid_history_uf_c3_std_2\n",
      "qid_history_uf_c4_mean qid_history_uf_c4_mean_2\n",
      "qid_history_uf_c4_std qid_history_uf_c4_std_2\n",
      "qid_history_uf_c5_mean qid_history_uf_c5_mean_2\n",
      "qid_history_uf_c5_std qid_history_uf_c5_std_2\n",
      "qid_history_score_mean qid_history_score_mean_2\n",
      "qid_history_score_std qid_history_score_std_2\n",
      "day day_2\n",
      "dup num:  27\n",
      "287\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "# t1 = []\n",
    "# n_dup = 0\n",
    "# for c in train.columns:\n",
    "#     if c not in t1:\n",
    "#         t1.append(c)\n",
    "#     else:\n",
    "#         t1.append(c+'_2')\n",
    "#         print(c, c+'_2')\n",
    "#         n_dup += 1\n",
    "# print('dup num: ', n_dup)\n",
    "# print('=================')\n",
    "\n",
    "# t2 = []\n",
    "# n_dup = 0\n",
    "# for c in test.columns:\n",
    "#     if c not in t2:\n",
    "#         t2.append(c)\n",
    "#     else:\n",
    "#         t2.append(c+'_2')\n",
    "#         print(c, c+'_2')\n",
    "#         n_dup += 1\n",
    "# print('dup num: ', n_dup)\n",
    "# #         print(c+'_2')\n",
    "# print(len(t1))\n",
    "# print(len(t2))\n",
    "\n",
    "# train.columns = t1\n",
    "# test.columns = t2\n",
    "# del train['day_2'], test['day_2']\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train['week'] = train['day']%7\n",
    "# test['week'] = test['day']%7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 07:03:35,685] INFO in <ipython-input-23-25a44eae3c1c>: train (9489162, 305)\n",
      "[2019-12-14 07:03:42,273] INFO in <ipython-input-23-25a44eae3c1c>: test (1141683, 304)\n"
     ]
    }
   ],
   "source": [
    "# 加载 invete feature 1\n",
    "# t1 = pd.read_csv(f'{feature_path}/train_invite_feature.txt', sep='\\t')\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/test_invite_feature.txt', sep='\\t')\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 07:04:50,430] INFO in <ipython-input-24-898ae5923687>: train (9489162, 333)\n",
      "[2019-12-14 07:04:58,287] INFO in <ipython-input-24-898ae5923687>: test (1141683, 332)\n"
     ]
    }
   ],
   "source": [
    "# 加载 invete feature 2\n",
    "# t1 = pd.read_csv(f'{feature_path}/train_invite_feature_2.txt', sep='\\t')\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/test_invite_feature_2.txt', sep='\\t')\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 07:06:27,612] INFO in <ipython-input-25-dff71a8a2332>: train (9489162, 357)\n",
      "[2019-12-14 07:06:36,460] INFO in <ipython-input-25-dff71a8a2332>: test (1141683, 356)\n"
     ]
    }
   ],
   "source": [
    "# 加载 kfold topic feature, QU\n",
    "# t1 = pd.read_csv(f'{feature_path}/train_kfold_topic_feature.txt', sep='\\t')\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/test_kfold_topic_feature.txt', sep='\\t')\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 07:08:26,674] INFO in <ipython-input-26-35a09705a8ca>: train (9489162, 373)\n",
      "[2019-12-14 07:08:34,000] INFO in <ipython-input-26-35a09705a8ca>: test (1141683, 372)\n"
     ]
    }
   ],
   "source": [
    "# 加载 user kfold topic feature，UU\n",
    "# t1 = pd.read_csv(f'{feature_path}/train_kfold_ut_feature.txt', sep='\\t')\n",
    "# train = pd.concat([train, t1], axis=1)\n",
    "# logging.info(\"train %s\", train.shape)\n",
    "\n",
    "# t1 = pd.read_csv(f'{feature_path}/test_kfold_ut_feature.txt', sep='\\t')\n",
    "# test = pd.concat([test, t1], axis=1)\n",
    "# logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "\n",
    "# t = train.dtypes\n",
    "# for x in t[t == 'int64'].index:\n",
    "#     train[x] = train[x].astype('int32')\n",
    "# for x in t[t == 'float64'].index:\n",
    "#     train[x] = train[x].astype('float32')\n",
    "\n",
    "# t = test.dtypes\n",
    "# for x in t[t == 'int64'].index:\n",
    "#     test[x] = test[x].astype('int32')\n",
    "# for x in t[t == 'float64'].index:\n",
    "#     test[x] = test[x].astype('float32')\n",
    "\n",
    "# pickle.dump(train, open(f'{feature_path}/train_373.pkl', 'wb'))\n",
    "# pickle.dump(test, open(f'{feature_path}/test_373.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(f'{feature_path}/train_373.pkl', 'rb'))\n",
    "train = pickle.load(open(f'{feature_path}/test_373.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-14 07:35:51,491] INFO in <ipython-input-34-a43802a9c4ae>: user (1931654, 14)\n",
      "[2019-12-14 07:35:55,675] INFO in <ipython-input-34-a43802a9c4ae>: user unq uid       1931654\n",
      "gender          3\n",
      "freq            5\n",
      "uf_b1           2\n",
      "uf_b2           2\n",
      "uf_b3           2\n",
      "uf_b4           2\n",
      "uf_b5           2\n",
      "uf_c1        2561\n",
      "uf_c2         291\n",
      "uf_c3         428\n",
      "uf_c4        1556\n",
      "uf_c5           2\n",
      "score         732\n",
      "dtype: int64\n",
      "[2019-12-14 07:35:55,681] INFO in <ipython-input-34-a43802a9c4ae>: user cat ['gender', 'freq', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5']\n",
      "[2019-12-14 07:35:56,732] INFO in <ipython-input-34-a43802a9c4ae>: encode gender\n",
      "[2019-12-14 07:35:57,738] INFO in <ipython-input-34-a43802a9c4ae>: encode freq\n",
      "[2019-12-14 07:35:58,679] INFO in <ipython-input-34-a43802a9c4ae>: encode uf_c1\n",
      "[2019-12-14 07:35:59,617] INFO in <ipython-input-34-a43802a9c4ae>: encode uf_c2\n",
      "[2019-12-14 07:36:00,492] INFO in <ipython-input-34-a43802a9c4ae>: encode uf_c3\n",
      "[2019-12-14 07:36:01,333] INFO in <ipython-input-34-a43802a9c4ae>: encode uf_c4\n",
      "[2019-12-14 07:36:02,144] INFO in <ipython-input-34-a43802a9c4ae>: encode uf_c5\n",
      "[2019-12-14 07:36:02,145] INFO in <ipython-input-34-a43802a9c4ae>: encoding qid...\n",
      "[2019-12-14 07:36:46,482] INFO in <ipython-input-34-a43802a9c4ae>: add qid_enc\n",
      "[2019-12-14 07:36:46,484] INFO in <ipython-input-34-a43802a9c4ae>: encoding uid...\n",
      "[2019-12-14 07:37:03,599] INFO in <ipython-input-34-a43802a9c4ae>: add uid_enc\n",
      "[2019-12-14 07:44:47,382] INFO in <ipython-input-34-a43802a9c4ae>: train shape (9489162, 388), test shape (1141683, 387)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载用户\n",
    "# user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "# user.columns = ['uid', 'gender', 'freq', 'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "#                 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5',  'score', 'follow_topic', 'inter_topic']\n",
    "\n",
    "# del user['follow_topic'], user['inter_topic']\n",
    "# logging.info(\"user %s\", user.shape)\n",
    "\n",
    "# unq = user.nunique()\n",
    "# logging.info(\"user unq %s\", unq)\n",
    "\n",
    "# for x in unq[unq == 1].index:\n",
    "#     del user[x]\n",
    "#     logging.info('del unq==1 %s', x)\n",
    "\n",
    "# t = user.dtypes\n",
    "# cats = [x for x in t[t == 'object'].index if x not in ['follow_topic', 'inter_topic', 'uid']]\n",
    "# logging.info(\"user cat %s\", cats)\n",
    "\n",
    "# for d in cats:\n",
    "#     lb = LabelEncoder()\n",
    "#     user[d] = lb.fit_transform(user[d])\n",
    "#     logging.info('encode %s', d)\n",
    "    \n",
    "# logging.info('encoding qid...')    \n",
    "# q_lb = LabelEncoder()\n",
    "# q_lb.fit(list(train['qid'].astype(str).values) + list(test['qid'].astype(str).values))\n",
    "# train['qid_enc'] = q_lb.transform(train['qid'])\n",
    "# test['qid_enc'] = q_lb.transform(test['qid'])\n",
    "# logging.info('add qid_enc')\n",
    "\n",
    "# logging.info('encoding uid...')\n",
    "# u_lb = LabelEncoder()\n",
    "# u_lb.fit(user['uid'])\n",
    "# train['uid_enc'] = u_lb.transform(train['uid'])\n",
    "# test['uid_enc'] = u_lb.transform(test['uid'])\n",
    "# logging.info('add uid_enc')\n",
    "\n",
    "# # merge user\n",
    "# train = pd.merge(train, user, on='uid', how='left')\n",
    "# test = pd.merge(test, user, on='uid', how='left')\n",
    "# logging.info(\"train shape %s, test shape %s\", train.shape, test.shape)\n",
    "\n",
    "# del user\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户问题等信息 encoding\n",
    "t1 = pd.read_pickle(f'{feature_path}/train_member_basic_feature.pkl')\n",
    "print(t1.keys())\n",
    "t1 = t1.reset_index(drop= True)\n",
    "train = train[[f for f in train.keys() if f not in t1.keys()]]\n",
    "train = train.reset_index(drop= True)\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "\n",
    "t1 = pd.read_pickle(f'{feature_path}/test_member_basic_feature.pkl')\n",
    "t1 = t1.reset_index(drop= True)\n",
    "test = test[[f for f in test.keys() if f not in t1.keys()]]\n",
    "test = test.reset_index(drop= True)\n",
    "test = pd.concat([test, t1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat((train, test), axis=0, sort=True)\n",
    "len_train = len(train)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(f'{feature_path}/count_features.txt', sep='\\t')\n",
    "print(len(t1), len(data))\n",
    "data = data[[f for f in data.keys() if f not in t1.keys()]]\n",
    "t1 = t1.reset_index(drop= True)\n",
    "data = data.reset_index(drop= True)\n",
    "data = pd.concat([data, t1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count 特征\n",
    "# count_feat = ['uid_enc', 'qid_enc', 'gender', 'freq', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5']\n",
    "# for feat in count_feat:\n",
    "#     logging.info('counting %s', feat)\n",
    "#     col_name = '{}_count'.format(feat)\n",
    "#     data[col_name] = data[feat].map(data[feat].value_counts().astype(int))\n",
    "#     data.loc[data[col_name] < 2, feat] = -1\n",
    "#     data[feat] += 1\n",
    "#     data[col_name] = data[feat].map(data[feat].value_counts().astype(int))\n",
    "#     data[col_name] = (data[col_name] - data[col_name].min()) / (data[col_name].max() - data[col_name].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feat = ['label', 'uid', 'qid', 'dt']\n",
    "drop_feat += ['gender_count','qid_enc_day_diff_iq_day_mean','qid_enc_day_diff_iq_day_median','uid_enc_day_score_mean',\n",
    "              'uid_enc_day_score_median','uf_b1_day_diff_iq_hour_mean','uf_b2_day_diff_iq_hour_mean','uf_b3_day_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_day_mean','qid_enc_day_hour_diff_iq_day_median','qid_enc_day_hour_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_hour_median','uid_enc_day_hour_score_mean','uid_enc_day_hour_score_median',\n",
    "              'uid_enc_qid_enc_score_mean','uid_enc_qid_enc_score_median','uid_enc_wk_score_mean','uid_enc_wk_score_median',\n",
    "              'freq_wk_score_median','freq_wk_diff_iq_hour_mean','gender_wk_diff_iq_day_median','gender_wk_diff_iq_hour_mean',\n",
    "              'uf_b1_wk_diff_iq_day_median','uf_b1_wk_diff_iq_hour_mean','uf_b2_wk_score_median','uf_b2_wk_diff_iq_day_median',\n",
    "              'uf_b2_wk_diff_iq_hour_mean','uf_b3_wk_diff_iq_hour_mean','uf_b4_wk_score_median','uf_b4_wk_diff_iq_day_median',\n",
    "              'uf_b4_wk_diff_iq_hour_mean','uf_b5_wk_diff_iq_day_median','uf_b5_wk_diff_iq_hour_mean','uf_c5_wk_diff_iq_hour_mean']\n",
    "# drop_feat += ['u_is_rec_mean', 'u_reci_uncheer_mean', 'q_is_dest_sum', 'u_reci_uncheer_sum', 'u_is_rec_max', \n",
    "#              'u_is_dest_mean','q_reci_uncheer_mean', 'q_reci_uncheer_sum', 'u_is_dest_sum', 'q_is_dest_max',\n",
    "#              'q_reci_uncheer_max', 'u_reci_tks_max', 'q_reci_mark_max','u_reci_dis_max', 'q_has_video_mean',\n",
    "#              'q_reci_no_help_mean', 'count_u_topic', 'u_has_video_mean', 'q_reci_dis_sum', 'q_reci_mark_sum',\n",
    "#              'q_reci_tks_sum','q_reci_tks_max','q_reci_dis_max','u_reci_mark_max','q_is_good_mean',\n",
    "#              'q_reci_no_help_sum', 'q_reci_xxx_max', 'u_reci_xxx_max','u_reci_no_help_sum','u_reci_xxx_sum',\n",
    "#               'u_is_good_mean','q_reci_no_help_max','u_has_img_max','u_is_good_sum','u_reci_no_help_max',\n",
    "#               'u_has_video_sum','uf_b5','q_reci_xxx_sum','q_is_good_sum','q_has_img_max','q_has_video_sum',\n",
    "#               'q_has_video_max','u_has_video_max','q_is_good_max','q_is_rec_max','u_is_good_max',\n",
    "#               'q_is_dest_mean','u_reci_uncheer_max','uf_c5_count','u_is_dest_max','q_is_rec_mean',\n",
    "#               'q_is_rec_sum','u_is_rec_sum', 'q_reci_xxx_mean','u_reci_xxx_mean','u_reci_comment_max',\n",
    "#               'q_reci_comment_sum','u_reci_cheer_max','u_reci_dis_sum','u_reci_tks_sum','q_has_img_sum',\n",
    "#               'q_reci_comment_max','q_reci_cheer_max','u_reci_no_help_mean','u_has_img_sum','u_reci_mark_sum']\n",
    "\n",
    "feature_with_day = [x for x in data.columns if x not in drop_feat]\n",
    "feature_cols = [x for x in data.columns if x not in drop_feat+['day']]\n",
    "# feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"feature size %s\", len(feature_cols))\n",
    "\n",
    "X_train_all = data.iloc[:len_train][feature_with_day]\n",
    "y_train_all = data.iloc[:len_train]['label']\n",
    "X_test = data.iloc[len_train:]\n",
    "assert len(X_test) == sub_size\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X=X_train_all, y=y_train_all)):\n",
    "    break\n",
    "\n",
    "X_train, X_val, y_train, y_val = X_train_all.iloc[train_idx][feature_cols], X_train_all.iloc[val_idx][feature_cols], \\\n",
    "                                 y_train_all.iloc[train_idx], \\\n",
    "                                 y_train_all.iloc[val_idx]\n",
    "del X_train_all\n",
    "\n",
    "# X_train = X_train_all.loc[X_train_all['day']<3867, feature_cols]\n",
    "# X_val = X_train_all.loc[X_train_all['day']==3867, feature_cols]\n",
    "# y_train = y_train_all[X_train_all['day']<3867]\n",
    "# y_val = y_train_all[X_train_all['day']==3867]\n",
    "# del X_train_all\n",
    "\n",
    "logging.info(\"train shape %s, val shape %s, test shape %s\", X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb = LGBMClassifier(n_estimators=2000, num_leaves=256, n_jobs=-1, objective='binary', learning_rate=0.01,\n",
    "#                            seed=1000, silent=True, max_bin=425, subsample_for_bin=50000, min_split_gain=0,\n",
    "#                           min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1,\n",
    "#                           colsample_bytree=1, reg_alpha=3, reg_lambda=5)\n",
    "model_lgb = LGBMClassifier(n_estimators=2000, n_jobs=-1, objective='binary', seed=1000, silent=True)\n",
    "model_lgb.fit(X_train, y_train,         \n",
    "              eval_metric=['logloss', 'auc'],\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_lgb, open('./model/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label'] = model_lgb.predict_proba(X_test[feature_cols])[:, 1]\n",
    "sub.to_csv('./result/2000_0.882585.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': feature_cols, 'imp': model_lgb.feature_importances_})\n",
    "fi['rate'] = fi['imp'] / fi['imp'].sum()\n",
    "fi_sorted = fi.sort_values(by='rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted.to_csv('./fi/fi.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(fi_sorted, open('./feature_importance.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted[-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
