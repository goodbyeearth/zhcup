{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import gc\n",
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 话题向量\n",
    "# topic_table = pd.read_csv(f'{base_path}/topic_vectors_64d.txt', sep='\\t', header=None)\n",
    "# topic_table.columns = ['topic', 'vec']\n",
    "\n",
    "# def str2vec(s):\n",
    "#     tmp = s.split(' ')\n",
    "#     res = []\n",
    "#     for num in tmp:\n",
    "#         res.append(float(num))\n",
    "#     return res\n",
    "\n",
    "# topic_table['vec'] = topic_table['vec'].apply(str2vec)\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(topic_table, open(f'{base_path}/topic_vec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_table = pickle.load(open(f'{base_path}/topic_vec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 04:26:56,821] INFO in <ipython-input-4-26315363bfb1>: ques (1829900, 2)\n"
     ]
    }
   ],
   "source": [
    "# 问题\n",
    "ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t', usecols=[0, 6])\n",
    "ques.columns = ['qid', 'topic']\n",
    "\n",
    "logging.info(\"ques %s\", ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "ans = pd.read_csv(f'{base_path}/answer_info_0926.txt', header=None, sep='\\t', usecols=[0, 1, 2, 3])\n",
    "ans.columns = ['aid', 'qid', 'uid', 'ans_dt']\n",
    "logging.info(\"ans %s\", ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[0][1:]))\n",
    "\n",
    "ans['day'] = extract_day(ans['ans_dt'])\n",
    "del ans['ans_dt']\n",
    "logging.info(\"ans %s\", ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans 对应的问题的话题\n",
    "ans = pd.merge(ans, ques, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4折统计\n",
    "def fold_fn(x):\n",
    "    if 3838<=x<=3846:\n",
    "        return 0\n",
    "    if 3847<=x<=3853:\n",
    "        return 1\n",
    "    if 3854<=x<=3860:\n",
    "        return 2\n",
    "    if 3861<=x<=3867:\n",
    "        return 3\n",
    "    else:\n",
    "        return -1     # 更前的一个月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "ans['fold'] = ans['day'].apply(fold_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_with_topic = ans[ans['topic']!='-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_topic_0 = ans_with_topic[ans_with_topic['fold']!=0].groupby('uid')['topic'].agg(','.join)\n",
    "logging.info('fold 0, user num: %s', len(user_topic_0))\n",
    "\n",
    "user_topic_1 = ans_with_topic[ans_with_topic['fold']!=1].groupby('uid')['topic'].agg(','.join)\n",
    "logging.info('fold 1, user num: %s', len(user_topic_1))\n",
    "\n",
    "user_topic_2 = ans_with_topic[ans_with_topic['fold']!=2].groupby('uid')['topic'].agg(','.join)\n",
    "logging.info('fold 2, user num: %s', len(user_topic_2))\n",
    "\n",
    "user_topic_3 = ans_with_topic[ans_with_topic['fold']!=3].groupby('uid')['topic'].agg(','.join)\n",
    "logging.info('fold 3, user num: %s', len(user_topic_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# user_topic_all = ans.groupby('uid')['topic'].agg(','.join)  # 给 test 用\n",
    "# pickle.dump(user_topic_all, open(f'{base_path}/user_topic.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_topic_all = pickle.load(open(f'{base_path}/user_topic.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "def compress_data(df):\n",
    "    t = df.dtypes\n",
    "    for x in t[t == 'int64'].index:\n",
    "        df[x] = df[x].astype('int32')\n",
    "\n",
    "    for x in t[t == 'float64'].index:\n",
    "        df[x] = df[x].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/((np.linalg.norm(vec1)*(np.linalg.norm(vec2))))\n",
    "\n",
    "def eucl_sim(vec1, vec2):\n",
    "    return np.linalg.norm(np.array(vec1)-np.array(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户在其他时期回答的话题统计特征\n",
    "def user_topic_stat(u_topic):\n",
    "    n_most_common = 20     # 截取回答次数前面的话题\n",
    "        \n",
    "    if u_topic == '-1':\n",
    "        return ([0] + [np.nan]*15)\n",
    "    \n",
    "    u_topic_2 = u_topic.split(',')\n",
    "    counter = Counter(u_topic_2)          \n",
    "    most_common_topic = counter.most_common(n_most_common)\n",
    "    \n",
    "    u_topic_most_com = []     # 前几个最常回答的话题\n",
    "    count_list = []           # 这几个常回答的话题对应的计数\n",
    "    for ut_c in most_common_topic: \n",
    "        u_topic_most_com.append(ut_c[0])\n",
    "        count_list.append(ut_c[1])\n",
    "    count_norm_list = np.array(list(count_list)) / np.sum(list(count_list))   # 这几个常回答话题对应的计数比例\n",
    "    \n",
    "    \"\"\"\n",
    "    用户话题的统计\n",
    "    \"\"\"\n",
    "    # count_list 的统计信息\n",
    "    count_u_topic = len(count_list)  \n",
    "    mean_u_topic = np.mean(count_list)\n",
    "    std_u_topic = np.std(count_list)  \n",
    "    sum_u_topic = np.sum(count_list)\n",
    "    max_u_topic = np.max(count_list)\n",
    "    min_u_topic = np.min(count_list)\n",
    "    # res 1\n",
    "    res = [count_u_topic, mean_u_topic, std_u_topic, sum_u_topic, max_u_topic, min_u_topic]\n",
    "    \n",
    "    if len(most_common_topic)>1:   # 2 以上，计算两两相似度\n",
    "        sim_list = []\n",
    "        sim_list_eucl = []\n",
    "        for i in range(len(most_common_topic)):\n",
    "            for j in range(i+1, len(most_common_topic)):\n",
    "                ut1_index = int(u_topic_most_com[i][1:]) - 1\n",
    "                ut2_index = int(u_topic_most_com[j][1:]) - 1\n",
    "                ut1 = topic_table.iloc[ut1_index]['vec']\n",
    "                ut2 = topic_table.iloc[ut2_index]['vec']\n",
    "\n",
    "                sim_list.append(cos_sim(ut1, ut2))\n",
    "                sim_list_eucl.append(eucl_sim(ut1, ut2))\n",
    "        # 余弦相似度\n",
    "        min_uu_sim = np.min(sim_list)\n",
    "        max_uu_sim = np.max(sim_list)\n",
    "        sum_uu_sim = np.sum(sim_list)\n",
    "        mean_uu_sim = np.mean(sim_list)\n",
    "        std_uu_sim = np.std(sim_list)\n",
    "        # 欧氏距离\n",
    "        min_uu_sim_eucl = np.min(sim_list_eucl)\n",
    "        max_uu_sim_eucl = np.max(sim_list_eucl)\n",
    "        sum_uu_sim_eucl = np.sum(sim_list_eucl)\n",
    "        mean_uu_sim_eucl = np.mean(sim_list_eucl)\n",
    "        std_uu_sim_eucl = np.std(sim_list_eucl)\n",
    "        # res 2\n",
    "        res += [min_uu_sim, max_uu_sim, sum_uu_sim, mean_uu_sim, std_uu_sim]\n",
    "        res += [min_uu_sim_eucl, max_uu_sim_eucl, sum_uu_sim_eucl, mean_uu_sim_eucl, std_uu_sim_eucl]\n",
    "    else:       # 用户回答个数为 1\n",
    "        # res 2\n",
    "        res += [np.nan] * 10\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "processes = 36   # 设置进程数\n",
    "\n",
    "def target_fn(series, path, num):\n",
    "    try:\n",
    "        logging.info('start to extract feature.')\n",
    "        df_from_series = series.reset_index()\n",
    "        t1 = df_from_series.apply(lambda x: user_topic_stat(x['topic']),axis=1, result_type='expand')\n",
    "        col_name = ['count_u_topic', 'mean_u_topic', 'std_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic', \n",
    "                    'min_uu_sim', 'max_uu_sim', 'sum_uu_sim', 'mean_uu_sim', 'std_uu_sim',\n",
    "                    'min_uu_sim_eucl', 'max_uu_sim_eucl', 'sum_uu_sim_eucl', 'mean_uu_sim_eucl', 'std_uu_sim_eucl']\n",
    "        t1.columns = col_name\n",
    "        res = pd.concat([df_from_series, t1], axis=1)\n",
    "        logging.info('extracting finish.')\n",
    "        \n",
    "        # 压缩数据\n",
    "        res = compress_data(res)\n",
    "        res.to_csv(f'{path}/user_topic_feature_{num}.txt', index=False, sep='\\t')\n",
    "        logging.info('file %s saving finish.', num)\n",
    "        del res\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(traceback.print_exc())\n",
    "        \n",
    "def multi_proc_1(series, frag_path):\n",
    "    import multiprocessing\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    len_data = len(series)\n",
    "    len_batch = len_data // processes\n",
    "    for i in range(processes):\n",
    "        start = i * len_batch\n",
    "        end = (i+1) * len_batch\n",
    "        if i == (processes-1):\n",
    "            end = len_data\n",
    "        tmp = series[start:end]\n",
    "        pool.apply_async(target_fn, (tmp, frag_path, i))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proc_1(user_topic_0, './temp_user_topic_stat/fold0')\n",
    "multi_proc_1(user_topic_1, './temp_user_topic_stat/fold1')\n",
    "multi_proc_1(user_topic_2, './temp_user_topic_stat/fold2')\n",
    "multi_proc_1(user_topic_3, './temp_user_topic_stat/fold3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合数据\n",
    "def concat_data(original_path, dest_path):\n",
    "    topic_feat = None\n",
    "    for i in range(processes):\n",
    "        d = pd.read_csv(f'{original_path}/user_topic_feature_{i}.txt', sep='\\t')\n",
    "        if topic_feat is None:\n",
    "            topic_feat = d\n",
    "        else:\n",
    "            topic_feat = pd.concat([topic_feat, d], axis=0, ignore_index=True)\n",
    "            \n",
    "    logging.info('topic feature, shape: %s, start saving', topic_feat.shape)\n",
    "    \n",
    "    topic_feat = compress_data(topic_feat)\n",
    "    topic_feat.to_csv(f'{dest_path}/topic_feature.txt', index=False, sep='\\t')\n",
    "    logging.info('saving done. From %s to %s', original_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data('./temp_user_topic_stat/fold0', f'./temp_user_topic_stat/dest0')\n",
    "concat_data('./temp_user_topic_stat/fold1', f'./temp_user_topic_stat/dest1')\n",
    "concat_data('./temp_user_topic_stat/fold2', f'./temp_user_topic_stat/dest2')\n",
    "concat_data('./temp_user_topic_stat/fold3', f'./temp_user_topic_stat/dest3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_topic_0, open(f'{base_path}/user_topic_fold_0.pkl', 'wb'))\n",
    "pickle.dump(user_topic_1, open(f'{base_path}/user_topic_fold_1.pkl', 'wb'))\n",
    "pickle.dump(user_topic_2, open(f'{base_path}/user_topic_fold_2.pkl', 'wb'))\n",
    "pickle.dump(user_topic_3, open(f'{base_path}/user_topic_fold_3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_stat_0 = pd.read_csv('./temp_user_topic_stat/dest0/topic_feature.txt', sep='\\t')\n",
    "ut_stat_1 = pd.read_csv('./temp_user_topic_stat/dest1/topic_feature.txt', sep='\\t')\n",
    "ut_stat_2 = pd.read_csv('./temp_user_topic_stat/dest2/topic_feature.txt', sep='\\t')\n",
    "ut_stat_3 = pd.read_csv('./temp_user_topic_stat/dest3/topic_feature.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "train['day'] = extract_day(train['dt'])\n",
    "del train['dt']\n",
    "logging.info(\"train %s\", train.shape)\n",
    "\n",
    "test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt']\n",
    "del test['dt']\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, ques, on='qid', how='left')\n",
    "test = pd.merge(test, ques, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train['fold'] = train['day'].apply(fold_fn)\n",
    "\n",
    "train['user_topic_kfold'] = -10000\n",
    "\n",
    "for i in range(4):\n",
    "    train.loc[train['fold']==i, 'user_topic_kfold'] = train[train['fold']==i]['uid'].map(ut_list[i]).fillna('-1')\n",
    "\n",
    "assert len(train[train['user_topic_kfold']==-10000])==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "user_topic = pickle.load(open(f'{base_path}/user_topic.pkl', 'rb'))       # 没有打折\n",
    "test['user_topic_kfold'] = test['uid'].map(user_topic).fillna('-1')    # 实际并不是 kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过去两个月的用户话题特征，给 test 数据用 \n",
    "multi_proc_1(user_topic, './temp_user_topic_stat/all')\n",
    "# 合数据\n",
    "concat_data('./temp_user_topic_stat/all', f'./temp_user_topic_stat/all_dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, 用户在其他时期回答的话题特征 merge 回来\n",
    "ut_stat_list = [ut_stat_0, ut_stat_1, ut_stat_2, ut_stat_3]\n",
    "ut_stat_feat = ['count_u_topic', 'mean_u_topic', 'std_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic', \n",
    "                'min_uu_sim', 'max_uu_sim','sum_uu_sim', 'mean_uu_sim', 'std_uu_sim', \n",
    "                'min_uu_sim_eucl', 'max_uu_sim_eucl', 'sum_uu_sim_eucl', 'mean_uu_sim_eucl', 'std_uu_sim_eucl']\n",
    "for f in ut_stat_feat:\n",
    "    train[f] = -10000\n",
    "for i in range(4):\n",
    "    train.loc[train['fold']==i, ut_stat_feat] = pd.merge(train.loc[train['fold']==i]['uid'], ut_stat_list[i], \n",
    "                                                         on='uid', how='left')[ut_stat_feat].values\n",
    "for f in ut_stat_feat:\n",
    "    assert len(train[train[f]==-10000]) == 0\n",
    "\n",
    "# 压缩数据\n",
    "int_feat = ['count_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic']\n",
    "train[int_feat] = train[int_feat].fillna(0).astype('int32')\n",
    "for f in ut_stat_feat:\n",
    "    if f not in int_feat:\n",
    "        train[f] = train[f].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, 用户在其他时期回答的话题特征 merge 回来\n",
    "ut_stat_all = pd.read_csv('./temp_user_topic_stat/all_dest/topic_feature.txt', sep='\\t')\n",
    "test = pd.merge(test, ut_stat_all, on='uid', how='left')\n",
    "\n",
    "# 压缩数据\n",
    "int_feat = ['count_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic']\n",
    "test[int_feat] = test[int_feat].fillna(0).astype('int32')\n",
    "for f in ut_stat_feat:\n",
    "    if f not in int_feat:\n",
    "        test[f] = test[f].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "train[ut_stat_feat].to_csv(f'{feature_path}/train_kfold_ut_feature.txt', index=False, sep='\\t')\n",
    "test[ut_stat_feat].to_csv(f'{feature_path}/test_kfold_ut_feature.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 04:28:17,368] INFO in <ipython-input-8-6d1f561dc942>: test2 (1141718, 2)\n"
     ]
    }
   ],
   "source": [
    "test2 = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "test2.columns = ['qid', 'uid', 'dt']\n",
    "del test2['dt']\n",
    "logging.info(\"test2 %s\", test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 04:28:31,350] INFO in <ipython-input-9-c7c956b507d1>: test2 (1141718, 19)\n"
     ]
    }
   ],
   "source": [
    "ut_stat_all = pd.read_csv('./temp_user_topic_stat/all_dest/topic_feature.txt', sep='\\t')\n",
    "test2 = pd.merge(test2, ut_stat_all, on='uid', how='left')\n",
    "logging.info(\"test2 %s\", test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "ut_stat_feat = ['count_u_topic', 'mean_u_topic', 'std_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic', \n",
    "                'min_uu_sim', 'max_uu_sim','sum_uu_sim', 'mean_uu_sim', 'std_uu_sim', \n",
    "                'min_uu_sim_eucl', 'max_uu_sim_eucl', 'sum_uu_sim_eucl', 'mean_uu_sim_eucl', 'std_uu_sim_eucl']\n",
    "int_feat = ['count_u_topic', 'sum_u_topic', 'max_u_topic', 'min_u_topic']\n",
    "test2[int_feat] = test2[int_feat].fillna(0).astype('int32')\n",
    "for f in ut_stat_feat:\n",
    "    if f not in int_feat:\n",
    "        test2[f] = test2[f].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test2[ut_stat_feat].to_csv(f'{feature_path}/test_kfold_ut_feature_2.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题话题与用户在其他时期回答的话题交叉特征\n",
    "def qu_topic_sim(q_topic, u_topic):\n",
    "    n_most_common = 20     # 截取回答次数前面的话题\n",
    "        \n",
    "    if (q_topic == '-1' or u_topic == '-1'):\n",
    "        return ([0]*4 + [np.nan]*20)\n",
    "    \n",
    "    q_topic_2 = q_topic.split(',')\n",
    "    u_topic_2 = u_topic.split(',')\n",
    "    counter = Counter(u_topic_2)          \n",
    "    most_common_topic = counter.most_common(n_most_common)\n",
    "    \n",
    "    u_topic_most_com = []     # 前几个最常回答的话题\n",
    "    count_list = []           # 这几个常回答的话题对应的计数\n",
    "    for ut_c in most_common_topic: \n",
    "        u_topic_most_com.append(ut_c[0])\n",
    "        count_list.append(ut_c[1])\n",
    "    count_norm_list = np.array(list(count_list)) / np.sum(list(count_list))   # 这几个常回答话题对应的计数比例\n",
    "    \n",
    "    \"\"\"\n",
    "    问题话题与用户话题交互统计\n",
    "    \"\"\"\n",
    "    \n",
    "    count_ut_in_qt_weighted = 0     # 问题话题在过往回答过的话题里的出现次数\n",
    "    count_ut_in_qt = 0             # 问题话题与过往回答过的话题的交集个数\n",
    "    count_norm_ut_in_qt = 0\n",
    "    # 余弦相似度\n",
    "    sim_list = []                   # 两两交叉计算相似度\n",
    "    sim_norm_list = []\n",
    "    # 欧氏距离\n",
    "    sim_list_eucl = []                   # 两两交叉计算相似度\n",
    "    sim_norm_list_eucl = []\n",
    "\n",
    "    for qt in q_topic_2:\n",
    "        q_topic_index = int(qt[1:]) - 1   # q_topic 索引\n",
    "        q_topic_vec = topic_table.iloc[q_topic_index]['vec']\n",
    "        for ut_c, count, count_norm in zip(most_common_topic, count_list, count_norm_list):  \n",
    "            u_topic_index = int(ut_c[0][1:]) - 1   # u_topic 索引\n",
    "            u_topic_vec = topic_table.iloc[u_topic_index]['vec']\n",
    "            if q_topic_index == u_topic_index:\n",
    "                count_ut_in_qt_weighted += ut_c[1]\n",
    "                count_ut_in_qt += 1\n",
    "                count_norm_ut_in_qt += count_norm\n",
    "            \n",
    "            sim = cos_sim(q_topic_vec, u_topic_vec)\n",
    "            sim_norm = sim*count_norm\n",
    "            sim_eucl = eucl_sim(q_topic_vec, u_topic_vec)\n",
    "            sim_norm_eucl = sim_eucl*count_norm\n",
    "            \n",
    "            sim_list.append(sim)\n",
    "            sim_norm_list.append(sim_norm)\n",
    "            sim_list_eucl.append(sim_eucl)\n",
    "            sim_norm_list_eucl.append(sim_norm_eucl)\n",
    "            \n",
    "    # 出现在问题和回答过的问题的话题，在问题话题中所占比例\n",
    "    rate_ut_in_qt = count_ut_in_qt / len(q_topic_2)\n",
    "    \n",
    "    # 余弦相似度\n",
    "    min_qu_sim = np.min(sim_list)\n",
    "    max_qu_sim = np.max(sim_list)\n",
    "    sum_qu_sim = np.sum(sim_list)\n",
    "    mean_qu_sim = np.mean(sim_list)\n",
    "    std_qu_sim = np.std(sim_list)\n",
    "    min_qu_sim_norm = np.min(sim_norm_list)\n",
    "    max_qu_sim_norm = np.max(sim_norm_list)\n",
    "    sum_qu_sim_norm = np.sum(sim_norm_list)\n",
    "    mean_qu_sim_norm = np.mean(sim_norm_list)\n",
    "    std_qu_sim_norm = np.std(sim_norm_list)\n",
    "    # 欧氏距离\n",
    "    min_qu_sim_eucl = np.min(sim_list_eucl)\n",
    "    max_qu_sim_eucl = np.max(sim_list_eucl)\n",
    "    sum_qu_sim_eucl = np.sum(sim_list_eucl)\n",
    "    mean_qu_sim_eucl = np.mean(sim_list_eucl)\n",
    "    std_qu_sim_eucl = np.std(sim_list_eucl)\n",
    "    min_qu_sim_norm_eucl = np.min(sim_norm_list_eucl)\n",
    "    max_qu_sim_norm_eucl = np.max(sim_norm_list_eucl)\n",
    "    sum_qu_sim_norm_eucl = np.sum(sim_norm_list_eucl)\n",
    "    mean_qu_sim_norm_eucl = np.mean(sim_norm_list_eucl)\n",
    "    std_qu_sim_norm_eucl = np.std(sim_norm_list_eucl)\n",
    "    \n",
    "    # 问题话题与用户回答话题的统计\n",
    "    res = [count_ut_in_qt_weighted, count_ut_in_qt, rate_ut_in_qt, count_norm_ut_in_qt]\n",
    "    res += [min_qu_sim, max_qu_sim, sum_qu_sim, mean_qu_sim, std_qu_sim]\n",
    "    res += [min_qu_sim_norm, max_qu_sim_norm, sum_qu_sim_norm, mean_qu_sim_norm, std_qu_sim_norm]\n",
    "    res += [min_qu_sim_eucl, max_qu_sim_eucl, sum_qu_sim_eucl, mean_qu_sim_eucl, std_qu_sim_eucl]\n",
    "    res += [min_qu_sim_norm_eucl, max_qu_sim_norm_eucl, sum_qu_sim_norm_eucl, mean_qu_sim_norm_eucl, std_qu_sim_norm_eucl]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "processes = 36   # 设置进程数\n",
    "\n",
    "def cross_qu_topic_feat(df, num, mode):\n",
    "    try:\n",
    "        logging.info('start to extract feature.')\n",
    "        t1 = df.apply(lambda x: qu_topic_sim(x['topic'], x['user_topic_kfold']), axis=1, result_type='expand')\n",
    "        col_name = ['qu_topic_count_weight', 'qu_topic_count', 'qu_topic_rate', 'qu_topic_count_norm',\n",
    "                  'min_sim', 'max_sim', 'sum_sim', 'mean_sim', 'std_sim', 'min_sim_norm', \n",
    "                  'max_sim_norm', 'sum_sim_norm', 'mean_sim_norm', 'std_sim_norm', \n",
    "                  'min_sim_eucl', 'max_sim_eucl', 'sum_sim_eucl', 'mean_sim_eucl', 'std_sim_eucl',\n",
    "                  'min_sim_norm_eucl', 'max_sim_norm_eucl', 'sum_sim_norm_eucl', 'mean_sim_norm_eucl', 'std_sim_norm_eucl']\n",
    "        t1.columns = col_name\n",
    "        logging.info('extracting finish.')\n",
    "        \n",
    "        # 压缩数据\n",
    "        t1 = compress_data(t1)\n",
    "        t1.to_csv(f'./temp/{mode}_kfold_topic_feature_{num}.txt', index=False, sep='\\t')\n",
    "        logging.info('file %s saving finish.', num)\n",
    "        del t1\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(traceback.print_exc())\n",
    "        \n",
    "def multi_proc(df, mode):\n",
    "    import multiprocessing\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    len_data = len(df)\n",
    "    len_batch = len_data // processes\n",
    "    for i in range(processes):\n",
    "        start = i * len_batch\n",
    "        end = (i+1) * len_batch\n",
    "        if i == (processes-1):\n",
    "            end = len_data\n",
    "        tmp = df[start:end]\n",
    "        pool.apply_async(cross_qu_topic_feat, (tmp, i, mode))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proc(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proc(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合数据\n",
    "\n",
    "# test\n",
    "topic_feat = None\n",
    "original_path = './temp'\n",
    "mode = 'test'\n",
    "for i in range(processes):\n",
    "    d = pd.read_csv(f'{original_path}/{mode}_kfold_topic_feature_{i}.txt', sep='\\t')\n",
    "    if topic_feat is None:\n",
    "        topic_feat = d\n",
    "    else:\n",
    "        topic_feat = pd.concat([topic_feat, d], axis=0, ignore_index=True)\n",
    "        \n",
    "logging.info('%s topic feature, shape: %s', mode, topic_feat.shape)\n",
    "\n",
    "topic_feat = compress_data(topic_feat)\n",
    "topic_feat.to_csv(f'{feature_path}/{mode}_kfold_topic_feature.txt', index=False, sep='\\t')\n",
    "logging.info('%s topic feature saved.', mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "topic_feat = None\n",
    "original_path = './temp'\n",
    "mode = 'train'\n",
    "for i in range(processes):\n",
    "    d = pd.read_csv(f'{original_path}/{mode}_kfold_topic_feature_{i}.txt', sep='\\t')\n",
    "    if topic_feat is None:\n",
    "        topic_feat = d\n",
    "    else:\n",
    "        topic_feat = pd.concat([topic_feat, d], axis=0, ignore_index=True)\n",
    "logging.info('%s topic feature, shape: %s', mode, topic_feat.shape)\n",
    "\n",
    "topic_feat = compress_data(topic_feat)\n",
    "topic_feat.to_csv(f'{feature_path}/{mode}_kfold_topic_feature.txt', index=False, sep='\\t')\n",
    "logging.info('%s topic feature saved.', mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2\n",
    "user_topic = pickle.load(open(f'{base_path}/user_topic.pkl', 'rb'))       # 没有打折\n",
    "test2['user_topic_kfold'] = test2['uid'].map(user_topic).fillna('-1')    # 实际并不是 kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "processes = 36   # 设置进程数\n",
    "\n",
    "def cross_qu_topic_feat(df, num, mode):\n",
    "    try:\n",
    "        logging.info('start to extract feature.')\n",
    "        t1 = df.apply(lambda x: qu_topic_sim(x['topic'], x['user_topic_kfold']), axis=1, result_type='expand')\n",
    "        col_name = ['qu_topic_count_weight', 'qu_topic_count', 'qu_topic_rate', 'qu_topic_count_norm',\n",
    "                  'min_sim', 'max_sim', 'sum_sim', 'mean_sim', 'std_sim', 'min_sim_norm', \n",
    "                  'max_sim_norm', 'sum_sim_norm', 'mean_sim_norm', 'std_sim_norm', \n",
    "                  'min_sim_eucl', 'max_sim_eucl', 'sum_sim_eucl', 'mean_sim_eucl', 'std_sim_eucl',\n",
    "                  'min_sim_norm_eucl', 'max_sim_norm_eucl', 'sum_sim_norm_eucl', 'mean_sim_norm_eucl', 'std_sim_norm_eucl']\n",
    "        t1.columns = col_name\n",
    "        logging.info('extracting finish.')\n",
    "        \n",
    "        # 压缩数据\n",
    "        t1 = compress_data(t1)\n",
    "        t1.to_csv(f'./temp/test2/{mode}_kfold_topic_feature_{num}.txt', index=False, sep='\\t')\n",
    "        logging.info('file %s saving finish.', num)\n",
    "        del t1\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(traceback.print_exc())\n",
    "        \n",
    "def multi_proc(df, mode):\n",
    "    import multiprocessing\n",
    "    pool = multiprocessing.Pool(processes=processes)\n",
    "    len_data = len(df)\n",
    "    len_batch = len_data // processes\n",
    "    for i in range(processes):\n",
    "        start = i * len_batch\n",
    "        end = (i+1) * len_batch\n",
    "        if i == (processes-1):\n",
    "            end = len_data\n",
    "        tmp = df[start:end]\n",
    "        pool.apply_async(cross_qu_topic_feat, (tmp, i, mode))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 04:30:06,547] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:06,724] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:06,931] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,160] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,320] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,452] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,560] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,675] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,786] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:07,923] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,247] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,379] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,608] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,605] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,701] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,778] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:08,886] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,016] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,135] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,342] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,421] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,501] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,655] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,792] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:09,885] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,001] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,093] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,179] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,316] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,411] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,507] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,669] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,763] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:10,892] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:11,012] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 04:30:11,137] INFO in <ipython-input-12-aa12bbe8f0d9>: start to extract feature.\n",
      "[2019-12-17 07:17:54,666] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:17:56,131] INFO in <ipython-input-12-aa12bbe8f0d9>: file 0 saving finish.\n",
      "[2019-12-17 07:19:43,906] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:19:45,438] INFO in <ipython-input-12-aa12bbe8f0d9>: file 5 saving finish.\n",
      "[2019-12-17 07:23:37,045] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:23:37,970] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:23:38,950] INFO in <ipython-input-12-aa12bbe8f0d9>: file 4 saving finish.\n",
      "[2019-12-17 07:23:39,416] INFO in <ipython-input-12-aa12bbe8f0d9>: file 2 saving finish.\n",
      "[2019-12-17 07:24:26,780] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:24:28,570] INFO in <ipython-input-12-aa12bbe8f0d9>: file 7 saving finish.\n",
      "[2019-12-17 07:24:44,148] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:24:45,649] INFO in <ipython-input-12-aa12bbe8f0d9>: file 1 saving finish.\n",
      "[2019-12-17 07:25:25,441] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:25:26,844] INFO in <ipython-input-12-aa12bbe8f0d9>: file 3 saving finish.\n",
      "[2019-12-17 07:25:42,125] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:25:43,664] INFO in <ipython-input-12-aa12bbe8f0d9>: file 8 saving finish.\n",
      "[2019-12-17 07:25:57,205] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:25:58,706] INFO in <ipython-input-12-aa12bbe8f0d9>: file 6 saving finish.\n",
      "[2019-12-17 07:27:21,648] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:27:23,021] INFO in <ipython-input-12-aa12bbe8f0d9>: file 12 saving finish.\n",
      "[2019-12-17 07:28:47,711] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:28:49,430] INFO in <ipython-input-12-aa12bbe8f0d9>: file 13 saving finish.\n",
      "[2019-12-17 07:28:56,538] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:28:57,931] INFO in <ipython-input-12-aa12bbe8f0d9>: file 11 saving finish.\n",
      "[2019-12-17 07:28:59,292] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:29:00,679] INFO in <ipython-input-12-aa12bbe8f0d9>: file 10 saving finish.\n",
      "[2019-12-17 07:30:33,457] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:30:34,858] INFO in <ipython-input-12-aa12bbe8f0d9>: file 9 saving finish.\n",
      "[2019-12-17 07:33:27,435] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:33:28,875] INFO in <ipython-input-12-aa12bbe8f0d9>: file 16 saving finish.\n",
      "[2019-12-17 07:34:39,758] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:34:41,138] INFO in <ipython-input-12-aa12bbe8f0d9>: file 14 saving finish.\n",
      "[2019-12-17 07:35:41,941] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:35:43,342] INFO in <ipython-input-12-aa12bbe8f0d9>: file 15 saving finish.\n",
      "[2019-12-17 07:36:46,941] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:36:48,346] INFO in <ipython-input-12-aa12bbe8f0d9>: file 21 saving finish.\n",
      "[2019-12-17 07:36:51,581] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:36:52,998] INFO in <ipython-input-12-aa12bbe8f0d9>: file 17 saving finish.\n",
      "[2019-12-17 07:37:20,958] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:37:22,364] INFO in <ipython-input-12-aa12bbe8f0d9>: file 22 saving finish.\n",
      "[2019-12-17 07:37:51,971] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:37:53,376] INFO in <ipython-input-12-aa12bbe8f0d9>: file 19 saving finish.\n",
      "[2019-12-17 07:38:17,628] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:38:19,048] INFO in <ipython-input-12-aa12bbe8f0d9>: file 34 saving finish.\n",
      "[2019-12-17 07:38:56,416] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:38:57,825] INFO in <ipython-input-12-aa12bbe8f0d9>: file 20 saving finish.\n",
      "[2019-12-17 07:39:02,472] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:39:03,886] INFO in <ipython-input-12-aa12bbe8f0d9>: file 28 saving finish.\n",
      "[2019-12-17 07:39:04,796] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:39:05,701] INFO in <ipython-input-12-aa12bbe8f0d9>: file 23 saving finish.\n",
      "[2019-12-17 07:39:43,794] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:39:44,653] INFO in <ipython-input-12-aa12bbe8f0d9>: file 32 saving finish.\n",
      "[2019-12-17 07:39:46,190] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:39:47,079] INFO in <ipython-input-12-aa12bbe8f0d9>: file 18 saving finish.\n",
      "[2019-12-17 07:40:12,333] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:13,196] INFO in <ipython-input-12-aa12bbe8f0d9>: file 33 saving finish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 07:40:17,973] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:18,818] INFO in <ipython-input-12-aa12bbe8f0d9>: file 26 saving finish.\n",
      "[2019-12-17 07:40:40,969] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:41,832] INFO in <ipython-input-12-aa12bbe8f0d9>: file 27 saving finish.\n",
      "[2019-12-17 07:40:48,570] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:49,467] INFO in <ipython-input-12-aa12bbe8f0d9>: file 35 saving finish.\n",
      "[2019-12-17 07:40:55,408] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:56,257] INFO in <ipython-input-12-aa12bbe8f0d9>: file 31 saving finish.\n",
      "[2019-12-17 07:40:56,426] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:40:57,271] INFO in <ipython-input-12-aa12bbe8f0d9>: file 24 saving finish.\n",
      "[2019-12-17 07:42:20,624] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:42:21,462] INFO in <ipython-input-12-aa12bbe8f0d9>: file 25 saving finish.\n",
      "[2019-12-17 07:42:55,597] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:42:56,423] INFO in <ipython-input-12-aa12bbe8f0d9>: file 30 saving finish.\n",
      "[2019-12-17 07:43:02,975] INFO in <ipython-input-12-aa12bbe8f0d9>: extracting finish.\n",
      "[2019-12-17 07:43:03,784] INFO in <ipython-input-12-aa12bbe8f0d9>: file 29 saving finish.\n"
     ]
    }
   ],
   "source": [
    "multi_proc(test2, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 07:46:38,471] INFO in <ipython-input-16-2d1cb0656709>: test topic feature, shape: (1141718, 24)\n",
      "[2019-12-17 07:47:09,062] INFO in <ipython-input-16-2d1cb0656709>: test topic feature saved.\n"
     ]
    }
   ],
   "source": [
    "# 新数据\n",
    "\n",
    "# test2\n",
    "topic_feat = None\n",
    "original_path = './temp/test2'\n",
    "mode = 'test'\n",
    "for i in range(processes):\n",
    "    d = pd.read_csv(f'{original_path}/{mode}_kfold_topic_feature_{i}.txt', sep='\\t')\n",
    "    if topic_feat is None:\n",
    "        topic_feat = d\n",
    "    else:\n",
    "        topic_feat = pd.concat([topic_feat, d], axis=0, ignore_index=True)\n",
    "        \n",
    "logging.info('%s topic feature, shape: %s', mode, topic_feat.shape)\n",
    "\n",
    "topic_feat = compress_data(topic_feat)\n",
    "topic_feat.to_csv(f'{feature_path}/new{mode}_kfold_topic_feature.txt', index=False, sep='\\t')\n",
    "logging.info('%s topic feature saved.', mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
