{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-09 07:52:01,114] INFO in <ipython-input-4-af34e33feb4f>: invite (9489162, 4)\n",
      "[2019-12-09 07:52:03,716] INFO in <ipython-input-4-af34e33feb4f>: test (1141683, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "logging.info(\"invite %s\", train.shape)\n",
    "\n",
    "test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt']\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[0][1:]))\n",
    "\n",
    "\n",
    "def extract_hour(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[1][1:]))\n",
    "\n",
    "train['day'] = extract_day(train['dt'])\n",
    "train['hour'] = extract_hour(train['dt'])\n",
    "\n",
    "test['day'] = extract_day(test['dt'])\n",
    "test['hour'] = extract_hour(test['dt'])\n",
    "\n",
    "del train['dt'], test['dt']\n",
    "\n",
    "train['week'] = train['day'] % 7\n",
    "test['week'] = test['day'] % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该 uid 在该小时、该天、该周几的被邀请数\n",
    "t1 = data.groupby(['uid', 'hour'])\n",
    "t2 = t1['qid'].count()\n",
    "t2.name = 'uid_hour_count'\n",
    "data = pd.merge(data, t2, on=['uid', 'hour'], how='left')\n",
    "\n",
    "t1 = data.groupby(['uid', 'day'])\n",
    "t2 = t1['qid'].count()\n",
    "t2.name = 'uid_day_count'\n",
    "data = pd.merge(data, t2, on=['uid', 'day'], how='left')\n",
    "\n",
    "t1 = data.groupby(['uid', 'week'])\n",
    "t2 = t1['qid'].count()\n",
    "t2.name = 'uid_week_count'\n",
    "data = pd.merge(data, t2, on=['uid', 'week'], how='left')\n",
    "\n",
    "# 该 qid 在该小时、该天、该周几的被邀请数\n",
    "t1 = data.groupby(['qid', 'hour'])\n",
    "t2 = t1['uid'].count()\n",
    "t2.name = 'qid_hour_count'\n",
    "data = pd.merge(data, t2, on=['qid', 'hour'], how='left')\n",
    "\n",
    "t1 = data.groupby(['qid', 'day'])\n",
    "t2 = t1['uid'].count()\n",
    "t2.name = 'qid_day_count'\n",
    "data = pd.merge(data, t2, on=['qid', 'day'], how='left')\n",
    "\n",
    "t1 = data.groupby(['qid', 'week'])\n",
    "t2 = t1['uid'].count()\n",
    "t2.name = 'qid_week_count'\n",
    "data = pd.merge(data, t2, on=['qid', 'week'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 uid、qid 统计用户偏好时段，小时、周的平均数、中位数、方差\n",
    "\n",
    "# uid\n",
    "t1 = data.groupby('uid')\n",
    "t2 = t1['hour'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['uid_hour_mean', 'uid_hour_median', 'uid_hour_std']\n",
    "data = pd.merge(data, t2, on='uid', how='left')\n",
    "\n",
    "t2 = t1['week'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['uid_week_mean', 'uid_week_median', 'uid_week_std']\n",
    "data = pd.merge(data, t2, on='uid', how='left')\n",
    "\n",
    "\n",
    "# qid\n",
    "t1 = data.groupby('qid')\n",
    "t2 = t1['hour'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['qid_hour_mean', 'qid_hour_median', 'qid_hour_std']\n",
    "data = pd.merge(data, t2, on='qid', how='left')\n",
    "\n",
    "t2 = t1['week'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['qid_week_mean', 'qid_week_median', 'qid_week_std']\n",
    "data = pd.merge(data, t2, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_feat = ['uid_hour_count', 'uid_day_count', 'uid_week_count', 'qid_hour_count', 'qid_day_count', 'qid_week_count']\n",
    "save_feat += ['uid_hour_mean', 'uid_hour_median', 'uid_hour_std']\n",
    "save_feat += ['uid_week_mean', 'uid_week_median', 'uid_week_std']\n",
    "save_feat += ['qid_hour_mean', 'qid_hour_median', 'qid_hour_std']\n",
    "save_feat += ['qid_week_mean', 'qid_week_median', 'qid_week_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "t = data[save_feat].dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[save_feat][:len(train)].to_csv(f'{feature_path}/train_invite_feature.txt', index=False, sep='\\t')\n",
    "data[save_feat][len(train):].to_csv(f'{feature_path}/test_invite_feature.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid_hour_max</th>\n",
       "      <th>uid_hour_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M1000000382</th>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M1000000983</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M1000003304</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M1000008978</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M1000009571</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M99999341</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M999995457</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M99999571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M999998695</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M999998888</th>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1419265 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             uid_hour_max  uid_hour_min\n",
       "uid                                    \n",
       "M1000000382            21            15\n",
       "M1000000983            21            11\n",
       "M1000003304            17            17\n",
       "M1000008978            17             8\n",
       "M1000009571            18            18\n",
       "...                   ...           ...\n",
       "M99999341               9             9\n",
       "M999995457             22             8\n",
       "M99999571               0             0\n",
       "M999998695             18             2\n",
       "M999998888             18            11\n",
       "\n",
       "[1419265 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uid 最早、最晚的钟点\n",
    "t1 = data.groupby('uid')\n",
    "t2 = t1['hour'].agg(['max', 'min'])\n",
    "t2.columns = ['uid_hour_max', 'uid_hour_min']\n",
    "data = pd.merge(data, t2, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uid \n",
    "t1 = data.groupby('uid')\n",
    "t2 = t1['day'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['uid_day_mean', 'uid_day_median', 'uid_day_std']\n",
    "data = pd.merge(data, t2, on='uid', how='left')\n",
    "\n",
    "# qid\n",
    "t1 = data.groupby('qid')\n",
    "t2 = t1['day'].agg(['mean', 'median', 'std'])\n",
    "t2.columns = ['qid_day_mean', 'qid_day_median', 'qid_day_std']\n",
    "data = pd.merge(data, t2, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid 被邀时间与自己偏好时间(mean, median)的时间差\n",
    "data['uid_diff_hour_hourmean'] = 12 - abs(abs(data['hour'] - data['uid_hour_mean']) - 12)\n",
    "data['uid_diff_hour_hourmedian'] = 12 - abs(abs(data['hour'] - data['uid_hour_median']) - 12)\n",
    "data['uid_diff_day_daymean'] = abs(data['day'] - data['uid_day_mean'])\n",
    "data['uid_diff_day_daymedian'] = abs(data['day'] - data['uid_day_median'])\n",
    "data['uid_diff_week_weekmean'] = 3.5 - abs(abs(data['week'] - data['uid_week_mean']) - 3.5)\n",
    "data['uid_diff_week_weekmedian'] = 3.5 - abs(abs(data['week'] - data['uid_week_median']) - 3.5)\n",
    "\n",
    "# qid 被邀时间与自己偏好时间(mean, median)的时间差\n",
    "data['qid_diff_hour_hourmean'] = 12 - abs(abs(data['hour'] - data['qid_hour_mean']) - 12)\n",
    "data['qid_diff_hour_hourmedian'] = 12 - abs(abs(data['hour'] - data['qid_hour_median']) - 12)\n",
    "data['qid_diff_day_daymean'] = abs(data['day'] - data['qid_day_mean'])\n",
    "data['qid_diff_day_daymedian'] = abs(data['day'] - data['qid_day_median'])\n",
    "data['qid_diff_week_weekmean'] = 3.5 - abs(abs(data['week'] - data['qid_week_mean']) - 3.5)\n",
    "data['qid_diff_week_weekmedian'] = 3.5 - abs(abs(data['week'] - data['qid_week_median']) - 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 02:48:13,470] INFO in <ipython-input-97-9063dae39e17>: ques (1829900, 3)\n"
     ]
    }
   ],
   "source": [
    "# 加载问题\n",
    "ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t')\n",
    "ques.columns = ['qid', 'q_dt', 'title_t1', 'title_t2', 'desc_t1', 'desc_t2', 'topic']\n",
    "del ques['title_t1'], ques['title_t2'], ques['desc_t1'], ques['desc_t2']\n",
    "logging.info(\"ques %s\", ques.shape)\n",
    "\n",
    "ques['q_day'] = extract_day(ques['q_dt'])\n",
    "ques['q_hour'] = extract_hour(ques['q_dt'])\n",
    "ques['q_week'] = ques['q_day'] % 7\n",
    "del ques['q_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, ques, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邀请与问题提出的时间差\n",
    "data['diff_iq_day'] = data['day'] - data['q_day']   \n",
    "data['diff_iq_hour'] = data['diff_iq_day'] * 24 + (data['hour'] - data['q_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 03:11:52,824] INFO in <ipython-input-113-4187da278e6d>: user (1931654, 16)\n"
     ]
    }
   ],
   "source": [
    "# 加载用户\n",
    "user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "user.columns = ['uid', 'gender', 'freq',\n",
    "                'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "                'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', \n",
    "                'score', 'follow_topic', 'inter_topic']\n",
    "\n",
    "logging.info(\"user %s\", user.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, user, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题对应每个性别的邀请数\n",
    "t1 = data.groupby(['qid', 'gender'])\n",
    "t2 = t1['uid'].count()\n",
    "t2.name = 'qid_gender_count'\n",
    "data = pd.merge(data, t2, on=['qid', 'gender'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 03:43:03,575] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_b1, min 1, max 4347\n",
      "[2019-12-03 03:45:33,611] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_b2, min 1, max 5221\n",
      "[2019-12-03 03:47:28,937] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_b3, min 1, max 6346\n",
      "[2019-12-03 03:48:48,085] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_b4, min 1, max 6293\n",
      "[2019-12-03 03:50:02,486] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_b5, min 1, max 6227\n",
      "[2019-12-03 03:51:18,195] INFO in <ipython-input-131-d56a60bbaa99>: feat uf_c5, min 1, max 6177\n"
     ]
    }
   ],
   "source": [
    "# 问题对应用户分类特征的邀请数\n",
    "for feat in ['uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', 'uf_c5']:  \n",
    "    t1 = data.groupby(['qid', feat])\n",
    "    t2 = t1['uid'].count()\n",
    "    t2.name = 'qid_' + feat + '_count'\n",
    "    data = pd.merge(data, t2, on=['qid', feat], how='left')\n",
    "    logging.info(\"feat %s, min %s, max %s\", feat, \n",
    "                 data['qid_' + feat + '_count'].min(), data['qid_' + feat + '_count'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow_topic 和 topic 交集个数\n",
    "def topic_intersection_count_1(follow_topic, ques_topic):\n",
    "    t_follow_topic = follow_topic.split(',')\n",
    "    t_ques_topic = ques_topic.split(',')\n",
    "    if t_follow_topic[0] == '-1' or t_ques_topic[0] == '-1':\n",
    "        return 0\n",
    "    return len(set(t_follow_topic) & set(t_ques_topic))\n",
    "\n",
    "# inter_topic 和 topic 交集个数\n",
    "def topic_intersection_count_2(inter_topic, ques_topic):\n",
    "    t_inter_topic = inter_topic.split(',')\n",
    "    t_ques_topic = ques_topic.split(',')\n",
    "    if t_inter_topic[0] == '-1' or t_ques_topic[0] == '-1':\n",
    "        return 0\n",
    "    count = 0\n",
    "    for t in t_inter_topic:\n",
    "        tt = t.split(':')\n",
    "        if tt[0] in t_ques_topic:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# inter_topic 和 topic 交集分数\n",
    "def topic_intersection_score(inter_topic, ques_topic):\n",
    "    t_inter_topic = inter_topic.split(',')\n",
    "    t_ques_topic = ques_topic.split(',')\n",
    "    if t_inter_topic[0] == '-1' or t_ques_topic[0] == '-1':\n",
    "        return 0\n",
    "    score = 0\n",
    "    for t in t_inter_topic:\n",
    "        tt = t.split(':')\n",
    "        if tt[0] in t_ques_topic:\n",
    "            score += float(tt[1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 05:49:48,638] INFO in <ipython-input-169-fa4a04937d75>: intersection_ft_count, max: 6\n",
      "[2019-12-03 06:11:57,893] INFO in <ipython-input-169-fa4a04937d75>: intersection_it_count, max: 5\n",
      "[2019-12-03 06:22:53,170] INFO in <ipython-input-169-fa4a04937d75>: intersection_it_score, max: inf\n"
     ]
    }
   ],
   "source": [
    "# follow_topic 和 topic 交集个数\n",
    "t1 = data.apply(lambda x: topic_intersection_count_1(x['follow_topic'], x['topic']), axis=1)\n",
    "t1.name = 'intersection_ft_count'\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('%s, max: %s', t1.name, t1.max())\n",
    "\n",
    "# inter_topic 和 topic 交集个数\n",
    "t1 = data.apply(lambda x: topic_intersection_count_2(x['inter_topic'], x['topic']), axis=1)\n",
    "t1.name = 'intersection_it_count'\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('%s, max: %s', t1.name, t1.max())\n",
    "\n",
    "# inter_topic 和 topic 交集分数\n",
    "t1 = data.apply(lambda x: topic_intersection_score(x['inter_topic'], x['topic']), axis=1)\n",
    "t1.name = 'intersection_it_score'\n",
    "data = pd.concat([data, t1], axis=1)\n",
    "logging.info('%s, max: %s', t1.name, t1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有两个 inf\n",
    "import numpy as np\n",
    "data['intersection_it_score'] = data['intersection_it_score'].replace(np.inf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid 被邀时间与自己偏好时间(mean, median)的时间差\n",
    "data['uid_diff_hour_hourmean'] = 12 - abs(abs(data['hour'] - data['uid_hour_mean']) - 12)\n",
    "data['uid_diff_hour_hourmedian'] = 12 - abs(abs(data['hour'] - data['uid_hour_median']) - 12)\n",
    "data['uid_diff_day_daymean'] = abs(data['day'] - data['uid_day_mean'])\n",
    "data['uid_diff_day_daymedian'] = abs(data['day'] - data['uid_day_median'])\n",
    "data['uid_diff_week_weekmean'] = 3.5 - abs(abs(data['week'] - data['uid_week_mean']) - 3.5)\n",
    "data['uid_diff_week_weekmedian'] = 3.5 - abs(abs(data['week'] - data['uid_week_median']) - 3.5)\n",
    "\n",
    "# qid 被邀时间与自己偏好时间(mean, median)的时间差\n",
    "data['qid_diff_hour_hourmean'] = 12 - abs(abs(data['hour'] - data['qid_hour_mean']) - 12)\n",
    "data['qid_diff_hour_hourmedian'] = 12 - abs(abs(data['hour'] - data['qid_hour_median']) - 12)\n",
    "data['qid_diff_day_daymean'] = abs(data['day'] - data['qid_day_mean'])\n",
    "data['qid_diff_day_daymedian'] = abs(data['day'] - data['qid_day_median'])\n",
    "data['qid_diff_week_weekmean'] = 3.5 - abs(abs(data['week'] - data['qid_week_mean']) - 3.5)\n",
    "data['qid_diff_week_weekmedian'] = 3.5 - abs(abs(data['week'] - data['qid_week_median']) - 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_feat = ['uid_hour_max', 'uid_hour_min']\n",
    "save_feat += ['uid_diff_hour_hourmean', 'uid_diff_hour_hourmedian', 'uid_diff_day_daymean', \n",
    "              'uid_diff_day_daymedian', 'uid_diff_week_weekmean', 'uid_diff_week_weekmedian',\n",
    "              'qid_diff_hour_hourmean', 'qid_diff_hour_hourmedian', 'qid_diff_day_daymean', \n",
    "              'qid_diff_day_daymedian', 'qid_diff_week_weekmean', 'qid_diff_week_weekmedian']\n",
    "save_feat += ['q_hour', 'q_week']\n",
    "save_feat += ['diff_iq_day', 'diff_iq_hour']\n",
    "save_feat += ['qid_gender_count', 'qid_uf_b1_count', 'qid_uf_b2_count', 'qid_uf_b3_count', \n",
    "              'qid_uf_b4_count', 'qid_uf_b5_count', 'qid_uf_c5_count']\n",
    "save_feat += ['intersection_ft_count', 'intersection_it_count', 'intersection_it_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "t = data[save_feat].dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[save_feat][:len(train)].to_csv(f'{feature_path}/train_invite_feature_2.txt', index=False, sep='\\t')\n",
    "data[save_feat][len(train):].to_csv(f'{feature_path}/test_invite_feature_2.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
