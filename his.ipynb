{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取历史跟label有关的feature\n",
    "import pandas as pd\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)\n",
    "\n",
    "base_path = './data'\n",
    "feature_path = './feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_invite_info_test2():\n",
    "    train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "    train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "    logging.info(\"invite %s\", train.shape)\n",
    "\n",
    "    test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "    test.columns = ['qid', 'uid', 'dt']\n",
    "    logging.info(\"test %s\", test.shape)\n",
    "\n",
    "    test1 = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "    test1.columns = ['qid', 'uid', 'dt']\n",
    "    logging.info(\"test %s\", test1.shape)\n",
    "\n",
    "    data = pd.concat([train,test,test1]).reset_index(drop=True)\n",
    "    data['day'] = data['dt'].apply(lambda x:int(x.split('-')[0].split('D')[1]))\n",
    "    data['hour'] = data['dt'].apply(lambda x:int(x.split('-')[1].split('H')[1]))\n",
    "    data['wk'] = data['day'] % 7\n",
    "    \n",
    "    del data['dt']\n",
    "   \n",
    "\n",
    "    # 加载问题信息\n",
    "    ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t')\n",
    "    ques.columns =  ['qid','create_time','title','title_cut','descrip','descrip_cut','bound_topic']\n",
    "    ques['q_day'] = ques.create_time.apply(lambda x: int(x.split('-')[0].split('D')[1]))\n",
    "    ques['q_hour'] = ques.create_time.apply(lambda x: int(x.split('-')[1].split('H')[1]))\n",
    "    # print(ques['qid','q_day','q_hour'].head(5))\n",
    "    data = pd.merge(data,ques[['qid','q_day','q_hour']],on='qid',how='left')\n",
    "    del ques\n",
    "    gc.collect()\n",
    "    data['diff_iq_day'] = data['day'] - data['q_day']\n",
    "    data['diff_iq_hour'] = data['day']*24 + data['hour'] - data['q_day']*24 - data['q_hour']\n",
    "\n",
    "    # 加载用户\n",
    "    user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "    user.columns = ['uid', 'gender' , 'freq',\n",
    "                    'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "                    'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', \n",
    "                    'score', 'follow_topic', 'inter_topic']\n",
    "    \n",
    "#     drop_feat = ['keyword','volume_level','heat_level','reg_type','reg_stage']\n",
    "#     user.drop(drop_feat,axis=1,inplace=True)\n",
    "    logging.info(\"user %s\", user.shape)\n",
    "    class_feat =  ['ffa','ffb','ffc','ffd','ffe','sex','freq']\n",
    "    user_feat = ['uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5','gender','freq']\n",
    "    user_feat_dict = {user_feat[i]:class_feat[i] for i in range(len(user_feat))}\n",
    "   \n",
    "    \n",
    "    data = pd.merge(data, user, on='uid', how='left')\n",
    "#     enc_dic = pickle.load(open(encoder_dic_file,'rb'))\n",
    "    for feat in user_feat:\n",
    "        # data[feat] = enc_dic[user_feat_dict[feat]].transform(data[feat])\n",
    "        lb = LabelEncoder()\n",
    "        lb.fit(data[feat])\n",
    "        data[feat] = lb.transform(data[feat])\n",
    "    del user\n",
    "    gc.collect()\n",
    "\n",
    "    # 加载qu_topic_count,qu_topic_count_weight 本次跑没有embedding信息\n",
    "    t1 = pd.read_csv(f'./feature/train_kfold_topic_feature.txt', sep='\\t', \n",
    "                 usecols=['qu_topic_count_weight', 'qu_topic_count'])\n",
    "    \n",
    "\n",
    "    t2 = pd.read_csv(f'./feature/test_kfold_topic_feature.txt', sep='\\t', \n",
    "                    usecols=['qu_topic_count_weight', 'qu_topic_count'])\n",
    "\n",
    "    t3 = pd.read_csv(f'./feature_test_2_ori/newtest_kfold_topic_feature.txt', sep='\\t', \n",
    "                    usecols=['qu_topic_count_weight', 'qu_topic_count'])\n",
    "    t = pd.concat([t1,t2,t3]).reset_index(drop=True)\n",
    "    print('tshape: ',t.shape)\n",
    "    print(data.shape)\n",
    "    data = pd.concat([data,t],axis=1)\n",
    "\n",
    "  \n",
    "    \n",
    "    t1 = pd.read_csv(f'./feature_test_2_ori/train2_invite_feature_2.txt', sep='\\t', \n",
    "                 usecols=['intersection_ft_count', 'intersection_it_count'])\n",
    "    \n",
    "\n",
    "    t2 = pd.read_csv(f'./feature_test_2_ori/test1_invite_feature_2.txt', sep='\\t', \n",
    "                    usecols=['intersection_ft_count', 'intersection_it_count'])\n",
    "\n",
    "    t3 = pd.read_csv(f'./feature_test_2_ori/test2_invite_feature_2.txt', sep='\\t', \n",
    "                    usecols=['intersection_ft_count', 'intersection_it_count'])\n",
    "    t = pd.concat([t1,t2,t3]).reset_index(drop=True)\n",
    "\n",
    "    data = pd.concat([data,t],axis=1)\n",
    "\n",
    "    return len(train),len(train)+len(test),data\n",
    "def group_ops(data,gp,feat,ops,alias=''):\n",
    "    t = data.groupby(gp)[feat].agg(ops).reset_index()\n",
    "    if type(gp) is not list:\n",
    "        gp = [gp]\n",
    "    if type(ops) is list:\n",
    "        t.columns = gp + [alias+'_'.join(gp)+'__'+'_'.join(x) for x in t.columns.ravel() if x[0] not in gp]\n",
    "    else:\n",
    "        t.columns = gp + [alias+'_'.join(gp)+'__'+'_'.join(x) for x in t.columns if x not in gp]\n",
    "    return t\n",
    "\n",
    "# last week : d-13~d-6            13\n",
    "# [{uid,qid}]_{label}_{mean,std,count,sum}  8\n",
    "# [qid_{user_feat}]_label_{mean,std,count,sum}  52\n",
    "# [{uid,qid}_{diff_iq_day,qu_topic_count,qu_topic_count_weight}]_{label}_{mean,std,count,sum} 24\n",
    "# [uid_{wk,hour}]_label_{mean,std,count,sum} 8\n",
    "# [{intersection,userfeats,diff_iq_hour,diff_iq_day}]_label_{mean,std,count,sum} 68\n",
    "#  = 13+8+52+24+8+68 = 73+32+68 = 173\n",
    "def generate_groups():\n",
    "    gps = []\n",
    "    user_feats = ['uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "         'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', \n",
    "         'score', 'freq', 'gender']\n",
    "\n",
    "    # gps.append(user_feats)\n",
    "    # gps.append(user_feats[:5])\n",
    "    # gps.append(user_feats[5:10])\n",
    "    gps.append(['uid'])\n",
    "    gps.append(['qid'])\n",
    "    for x in ['wk','hour']:\n",
    "        gps.append(['uid',x])\n",
    "    \n",
    "    for x in ['diff_iq_day','qu_topic_count','qu_topic_count_weight']:\n",
    "        gps.append(['qid',x])\n",
    "        gps.append(['uid',x])\n",
    "\n",
    "    for x in user_feats:\n",
    "        gps.append(['qid',x])\n",
    "        gps.append([x])\n",
    "\n",
    "    for x in ['intersection_ft_count','intersection_ft_count','diff_iq_hour','diff_iq_day']:\n",
    "        gps.append([x])\n",
    "\n",
    "    return gps\n",
    "\n",
    "def extract_lw_feature(data,mode=''):\n",
    "    res= []\n",
    "    gps = generate_groups()\n",
    "    for i in range(len(gps)):\n",
    "        res.append([])\n",
    "\n",
    "    ops = ['mean','std','count','sum']\n",
    "    print('last week ', mode)\n",
    "    if mode == 'test2':\n",
    "        start,end = 3868,3875\n",
    "    else:\n",
    "        start,end = 3851,3875\n",
    "\n",
    "    for d in range(start,end):\n",
    "        print('day ',d)\n",
    "        sel = data[(data.day >= d-13)&(data.day < d-6)]\n",
    "        gc.collect()\n",
    "        for i in range(len(gps)):\n",
    "            t = group_ops(sel,gps[i],['label'],ops)\n",
    "            t['day'] = d\n",
    "            res[i].append(t)\n",
    "    \n",
    "    # merge on data\n",
    "    print('start to merge')\n",
    "    for i in range(len(gps)):\n",
    "        print('working on ',i)\n",
    "        data = data.merge(pd.concat(res[i]),on=['day']+gps[i],how='left')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_h6_feature(data,mode=''):\n",
    "    res= []\n",
    "    gps = generate_groups()\n",
    "    for i in range(len(gps)):\n",
    "        res.append([])\n",
    "\n",
    "    ops = ['mean','std','count','sum']\n",
    "    print('less than 6d ',mode)\n",
    "    if mode == 'test2':\n",
    "        start,end = 3868,3875\n",
    "    else:\n",
    "        start,end = 3844,3875\n",
    "\n",
    "    for d in range(start,end):\n",
    "        print('day ',d)\n",
    "        sel = data[(data.day < d-6)]\n",
    "        window_size = d-6-3838\n",
    "        if len(sel) == 0:\n",
    "            print(f'day {d} windowsize = 0')\n",
    "            continue\n",
    "        else:\n",
    "            print(f'day {d} windowsize = {window_size}')\n",
    "            \n",
    "        gc.collect()\n",
    "        for i in range(len(gps)):\n",
    "            t = group_ops(sel,gps[i],['label'],ops)\n",
    "            t['day'] = d\n",
    "            for x in t.columns:\n",
    "                if 'count' in x:\n",
    "                    t[x] /= window_size\n",
    "            res[i].append(t)\n",
    "\n",
    "    # merge on data\n",
    "    print('start to merge')\n",
    "    for i in range(len(gps)):\n",
    "        print('working on ',i)\n",
    "        data = data.merge(pd.concat(res[i]),on=['day']+gps[i],how='left')\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_hd_feature(data,mode=''):\n",
    "    res= []\n",
    "    gps = generate_groups()\n",
    "    for i in range(len(gps)):\n",
    "        res.append([])\n",
    "\n",
    "    ops = ['mean','std','count','sum']\n",
    "\n",
    "    print('less than d ',mode)\n",
    "    if mode == 'test2':\n",
    "        start,end = 3868,3875\n",
    "    else:\n",
    "        start,end = 3839,3875\n",
    "\n",
    "    for d in range(start,end):\n",
    "        print('day ',d)\n",
    "        sel = data[(data.day < d)]\n",
    "        if d <= 3867 :\n",
    "            window_size = d-3838\n",
    "        else:\n",
    "            window_size = 3867-3838\n",
    "        if len(sel) == 0:\n",
    "            print(f'day {d} windowsize = 0')\n",
    "            continue\n",
    "        else:\n",
    "            print(f'day {d} windowsize = {window_size}')\n",
    "        gc.collect()\n",
    "        for i in range(len(gps)):\n",
    "            t = group_ops(sel,gps[i],['label'],ops)\n",
    "            t['day'] = d\n",
    "            for x in t.columns:\n",
    "                if 'count' in x:\n",
    "                    t[x] /= window_size\n",
    "            res[i].append(t)\n",
    "    \n",
    "    # merge on data\n",
    "    print('start to merge')\n",
    "    for i in range(len(gps)):\n",
    "        print('working on ',i)\n",
    "        data = data.merge(pd.concat(res[i]),on=['day']+gps[i],how='left')   \n",
    "\n",
    "    return data    \n",
    "    \n",
    "\n",
    "def extract_all():\n",
    "    # ltrain,data = load_invite_info()\n",
    "    # original_cols = list(data.columns)\n",
    "    # data = extract_lw_feature(data)\n",
    "    # after_cols = list(data.columns)\n",
    "    # data.drop(original_cols,axis=1,inplace=True)\n",
    "    # pickle.dump(data,open('./feature/new_history_lastweek.pkl','wb'),protocol=4)\n",
    "    # del data\n",
    "    # gc.collect()\n",
    "\n",
    "    ltrain,data = load_invite_info()\n",
    "    original_cols = list(data.columns)\n",
    "    data = extract_h6_feature(data)\n",
    "    after_cols = list(data.columns)\n",
    "    data.drop(original_cols,axis=1,inplace=True)\n",
    "    pickle.dump(data,open('./feature/new_history_ltd6.pkl','wb'),protocol=4)\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    ltrain,data = load_invite_info()\n",
    "    original_cols = list(data.columns)\n",
    "    data = extract_hd_feature(data)\n",
    "    after_cols = list(data.columns)\n",
    "    data.drop(original_cols,axis=1,inplace=True)\n",
    "    pickle.dump(data,open('./feature/new_history_ltd.pkl','wb'),protocol=4)\n",
    "\n",
    "def extract_all_test2():\n",
    "    ltrain,ltest,data = load_invite_info_test2()\n",
    "    original_cols = list(data.columns)\n",
    "    data = extract_lw_feature(data,'test2')\n",
    "    after_cols = list(data.columns)\n",
    "    data.drop(original_cols,axis=1,inplace=True)\n",
    "    pickle.dump(data.iloc[ltest:],open('./feature/new_history_lastweek_test2.pkl','wb'),protocol=4)\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    ltrain,ltest,data = load_invite_info_test2()\n",
    "    original_cols = list(data.columns)\n",
    "    data = extract_h6_feature(data,'test2')\n",
    "    after_cols = list(data.columns)\n",
    "    data.drop(original_cols,axis=1,inplace=True)\n",
    "    pickle.dump(data.iloc[ltest:],open('./feature/new_history_ltd6_test2.pkl','wb'),protocol=4)\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    ltrain,ltest,data = load_invite_info_test2()\n",
    "    original_cols = list(data.columns)\n",
    "    data = extract_hd_feature(data,'test2')\n",
    "    after_cols = list(data.columns)\n",
    "    data.drop(original_cols,axis=1,inplace=True)\n",
    "    pickle.dump(data.iloc[ltest:],open('./feature/new_history_ltd_test2.pkl','wb'),protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 10:32:46,379] INFO in <ipython-input-2-d390e72d4f3d>: invite (9489162, 4)\n",
      "[2019-12-17 10:32:47,847] INFO in <ipython-input-2-d390e72d4f3d>: test (1141683, 3)\n",
      "[2019-12-17 10:32:49,295] INFO in <ipython-input-2-d390e72d4f3d>: test (1141718, 3)\n",
      "/root/anaconda3/envs/zhcup/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "[2019-12-17 10:35:52,169] INFO in <ipython-input-2-d390e72d4f3d>: user (1931654, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshape:  (11772563, 2)\n",
      "(11772563, 25)\n",
      "less than d  test2\n",
      "day  3868\n",
      "day 3868 windowsize = 29\n",
      "day  3869\n",
      "day 3869 windowsize = 29\n",
      "day  3870\n",
      "day 3870 windowsize = 29\n",
      "day  3871\n",
      "day 3871 windowsize = 29\n",
      "day  3872\n",
      "day 3872 windowsize = 29\n",
      "day  3873\n",
      "day 3873 windowsize = 29\n",
      "day  3874\n",
      "day 3874 windowsize = 29\n",
      "start to merge\n",
      "working on  0\n",
      "working on  1\n",
      "working on  2\n",
      "working on  3\n",
      "working on  4\n",
      "working on  5\n",
      "working on  6\n",
      "working on  7\n",
      "working on  8\n",
      "working on  9\n",
      "working on  10\n",
      "working on  11\n",
      "working on  12\n",
      "working on  13\n",
      "working on  14\n",
      "working on  15\n",
      "working on  16\n",
      "working on  17\n",
      "working on  18\n",
      "working on  19\n",
      "working on  20\n",
      "working on  21\n",
      "working on  22\n",
      "working on  23\n",
      "working on  24\n",
      "working on  25\n",
      "working on  26\n",
      "working on  27\n",
      "working on  28\n",
      "working on  29\n",
      "working on  30\n",
      "working on  31\n",
      "working on  32\n",
      "working on  33\n",
      "working on  34\n",
      "working on  35\n",
      "working on  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/zhcup/lib/python3.6/site-packages/pandas/core/reshape/merge.py:1100: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on  37\n",
      "working on  38\n",
      "working on  39\n"
     ]
    }
   ],
   "source": [
    "ltrain,ltest,data = load_invite_info_test2()\n",
    "original_cols = list(data.columns)\n",
    "data = extract_hd_feature(data,'test2')\n",
    "after_cols = list(data.columns)\n",
    "data.drop(original_cols,axis=1,inplace=True)\n",
    "pickle.dump(data.iloc[ltest:],open('./feature_test_2_ori/new_history_ltd_test2.pkl','wb'),protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
