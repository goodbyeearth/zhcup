{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'\n",
    "newfeature_path = './feature_test_2_ori'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt']\n",
    "sub = test.copy()\n",
    "sub_size = len(sub)\n",
    "\n",
    "del test['dt']\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid feature\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/test2_qid_feature.pkl')\n",
    "t1 = t1.reset_index(drop= True)\n",
    "# test = test.reset_index(drop= True)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 history \n",
    "t1 = pd.read_pickle(f'{newfeature_path}/history_feature_test2.pkl')\n",
    "t1 = t1.reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_pickle(f'{newfeature_path}/history_feature1_test2.pkl')\n",
    "t1 = t1.reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_pickle(f'{newfeature_path}/history_feature2_test2.pkl')\n",
    "t1 = t1.reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 ans kfold feature\n",
    "all_col = ['day', 'hour', 'q_inv_kfold_mean', 'q_inv_kfold_sum', 'q_inv_kfold_std', 'q_inv_kfold_count', \n",
    "           'u_inv_kfold_mean', 'u_inv_kfold_sum', 'u_inv_kfold_std', 'u_inv_kfold_count', 'q_ans_kfold_count',\n",
    "           'u_ans_kfold_count', 'q_is_good_sum', 'q_is_good_max', 'q_is_good_mean', 'u_is_good_sum',\n",
    "           'u_is_good_max', 'u_is_good_mean', 'q_is_rec_sum', 'q_is_rec_max', 'q_is_rec_mean', 'u_is_rec_sum',\n",
    "           'u_is_rec_max', 'u_is_rec_mean', 'q_is_dest_sum', 'q_is_dest_max', 'q_is_dest_mean', \n",
    "           'u_is_dest_sum', 'u_is_dest_max', 'u_is_dest_mean', 'q_has_img_sum', 'q_has_img_max', \n",
    "           'q_has_img_mean', 'u_has_img_sum', 'u_has_img_max', 'u_has_img_mean', 'q_has_video_sum', \n",
    "           'q_has_video_max', 'q_has_video_mean', 'u_has_video_sum', 'u_has_video_max', 'u_has_video_mean',\n",
    "           'q_word_count_sum', 'q_word_count_max', 'q_word_count_mean', 'u_word_count_sum', 'u_word_count_max',\n",
    "           'u_word_count_mean', 'q_reci_cheer_sum', 'q_reci_cheer_max', 'q_reci_cheer_mean', 'u_reci_cheer_sum',\n",
    "           'u_reci_cheer_max', 'u_reci_cheer_mean', 'q_reci_uncheer_sum', 'q_reci_uncheer_max', \n",
    "           'q_reci_uncheer_mean', 'u_reci_uncheer_sum', 'u_reci_uncheer_max', 'u_reci_uncheer_mean', \n",
    "           'q_reci_comment_sum', 'q_reci_comment_max', 'q_reci_comment_mean', 'u_reci_comment_sum', \n",
    "           'u_reci_comment_max', 'u_reci_comment_mean', 'q_reci_mark_sum', 'q_reci_mark_max', \n",
    "           'q_reci_mark_mean', 'u_reci_mark_sum', 'u_reci_mark_max', 'u_reci_mark_mean', 'q_reci_tks_sum',\n",
    "           'q_reci_tks_max', 'q_reci_tks_mean', 'u_reci_tks_sum', 'u_reci_tks_max', 'u_reci_tks_mean',\n",
    "           'q_reci_xxx_sum', 'q_reci_xxx_max', 'q_reci_xxx_mean', 'u_reci_xxx_sum', 'u_reci_xxx_max', \n",
    "           'u_reci_xxx_mean', 'q_reci_no_help_sum', 'q_reci_no_help_max', 'q_reci_no_help_mean', \n",
    "           'u_reci_no_help_sum', 'u_reci_no_help_max', 'u_reci_no_help_mean', 'q_reci_dis_sum', \n",
    "           'q_reci_dis_max', 'q_reci_dis_mean', 'u_reci_dis_sum', 'u_reci_dis_max', 'u_reci_dis_mean', \n",
    "           'q_diff_qa_days_sum', 'q_diff_qa_days_max', 'q_diff_qa_days_mean', 'u_diff_qa_days_sum', \n",
    "           'u_diff_qa_days_max', 'u_diff_qa_days_mean']\n",
    "drop_col = ['u_is_rec_mean', 'u_reci_uncheer_mean', 'q_is_dest_sum', 'u_reci_uncheer_sum', 'u_is_rec_max', \n",
    "             'u_is_dest_mean','q_reci_uncheer_mean', 'q_reci_uncheer_sum', 'u_is_dest_sum', 'q_is_dest_max',\n",
    "             'q_reci_uncheer_max', 'u_reci_tks_max', 'q_reci_mark_max','u_reci_dis_max', 'q_has_video_mean',\n",
    "             'q_reci_no_help_mean', 'count_u_topic', 'u_has_video_mean', 'q_reci_dis_sum', 'q_reci_mark_sum',\n",
    "             'q_reci_tks_sum','q_reci_tks_max','q_reci_dis_max','u_reci_mark_max','q_is_good_mean',\n",
    "             'q_reci_no_help_sum', 'q_reci_xxx_max', 'u_reci_xxx_max','u_reci_no_help_sum','u_reci_xxx_sum',\n",
    "              'u_is_good_mean','q_reci_no_help_max','u_has_img_max','u_is_good_sum','u_reci_no_help_max',\n",
    "              'u_has_video_sum','uf_b5','q_reci_xxx_sum','q_is_good_sum','q_has_img_max','q_has_video_sum',\n",
    "              'q_has_video_max','u_has_video_max','q_is_good_max','q_is_rec_max','u_is_good_max',\n",
    "              'q_is_dest_mean','u_reci_uncheer_max','uf_c5_count','u_is_dest_max','q_is_rec_mean',\n",
    "              'q_is_rec_sum','u_is_rec_sum', 'q_reci_xxx_mean','u_reci_xxx_mean','u_reci_comment_max',\n",
    "              'q_reci_comment_sum','u_reci_cheer_max','u_reci_dis_sum','u_reci_tks_sum','q_has_img_sum',\n",
    "              'q_reci_comment_max','q_reci_cheer_max','u_reci_no_help_mean','u_has_img_sum','u_reci_mark_sum']\n",
    "use_col = list(set(all_col) - set(drop_col))\n",
    "\n",
    "\n",
    "t1 = pd.read_csv(f'{newfeature_path}/test2_kfold_feature.txt', sep='\\t', usecols=use_col)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 重命名\n",
    "t2 = []\n",
    "n_dup = 0\n",
    "for c in test.columns:\n",
    "    if c not in t2:\n",
    "        t2.append(c)\n",
    "    else:\n",
    "        t2.append(c+'_2')\n",
    "        print(c, c+'_2')\n",
    "        n_dup += 1\n",
    "print('dup num: ', n_dup)\n",
    "\n",
    "print(len(t2))\n",
    "test.columns = t2\n",
    "del test['day_2']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['week'] = test['day']%7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 invete feature 1\n",
    "t1 = pd.read_csv(f'{newfeature_path}/test2_invite_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 invete feature 2\n",
    "t1 = pd.read_csv(f'{newfeature_path}/test2_invite_feature_2.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 kfold topic feature, QU\n",
    "t1 = pd.read_csv(f'{newfeature_path}/newtest_kfold_topic_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 user kfold topic feature，UU\n",
    "t1 = pd.read_csv(f'{newfeature_path}/newtest_kfold_ut_feature.txt', sep='\\t')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    test[x] = test[x].astype('int32')\n",
    "for x in t[t == 'float64'].index:\n",
    "    test[x] = test[x].astype('float32')\n",
    "    \n",
    "pickle.dump(test, open(f'{feature_path}/newtest_372.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pickle.load(open(f'{feature_path}/newtest_372.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户问题等信息 encoding\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/test_member_basic_feature.pkl')\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计数特征\n",
    "t1 = pd.read_pickle(f'{newfeature_path}/count_features.pkl')[9489162:].reset_index(drop=True)\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "\n",
    "\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feat = ['label', 'uid', 'qid', 'dt']\n",
    "drop_feat += ['gender_count','qid_enc_day_diff_iq_day_mean','qid_enc_day_diff_iq_day_median','uid_enc_day_score_mean',\n",
    "              'uid_enc_day_score_median','uf_b1_day_diff_iq_hour_mean','uf_b2_day_diff_iq_hour_mean','uf_b3_day_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_day_mean','qid_enc_day_hour_diff_iq_day_median','qid_enc_day_hour_diff_iq_hour_mean',\n",
    "              'qid_enc_day_hour_diff_iq_hour_median','uid_enc_day_hour_score_mean','uid_enc_day_hour_score_median',\n",
    "              'uid_enc_qid_enc_score_mean','uid_enc_qid_enc_score_median','uid_enc_wk_score_mean','uid_enc_wk_score_median',\n",
    "              'freq_wk_score_median','freq_wk_diff_iq_hour_mean','gender_wk_diff_iq_day_median','gender_wk_diff_iq_hour_mean',\n",
    "              'uf_b1_wk_diff_iq_day_median','uf_b1_wk_diff_iq_hour_mean','uf_b2_wk_score_median','uf_b2_wk_diff_iq_day_median',\n",
    "              'uf_b2_wk_diff_iq_hour_mean','uf_b3_wk_diff_iq_hour_mean','uf_b4_wk_score_median','uf_b4_wk_diff_iq_day_median',\n",
    "              'uf_b4_wk_diff_iq_hour_mean','uf_b5_wk_diff_iq_day_median','uf_b5_wk_diff_iq_hour_mean','uf_c5_wk_diff_iq_hour_mean']\n",
    "drop_feat += ['uf_b2_max','uf_c5_max','dayuf_b2_max','qid_wk_uf_b3_median','qid_wk_uf_b1_max','dayuf_b1_max','qid_wk_uf_b2_median',\n",
    "              'dayuf_b1_min','dayuf_c5_min','uf_b2_median','qid_wk_uf_b4_min','dayuf_b4_min','uf_b5_min','uf_c5_min','qid_wk_uf_b1_min',\n",
    "              'qid_wk_uf_b5_max','uf_b1_max','dayuf_b3_max','dayuf_b2_min','uf_b4_max','uf_b3','uf_b5_max','qid_wk_uf_c5_median','uf_b4_min',\n",
    "              'uf_b2_min','dayuf_b5_min','qid_wk_uf_b2_min','qid_wk_uf_b4_median','qid_wk_uf_b3_max','qid_enc_day_score_mean',\n",
    "              'qid_wk_uf_c5_min','dayuf_b3_median','uf_b5_median','uf_b3_min','dayuf_b5_max','qid_wk_uf_b5_min','qid_wk_uf_b2_max',\n",
    "              'dayuf_b3_min','dayuf_b4_max','qid_enc_day_score_median','qid_wk_uf_b1_median','dayuf_b2_median','dayuf_c5_max',\n",
    "              'dayuf_b5_median','dayuf_c5_median']\n",
    "\n",
    "cate_feats = ['qid_enc', 'uid_enc', 'freq', 'gender', 'uf_b1', 'uf_b2', 'uf_b3', 'uf_b4', 'uf_b5', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5']\n",
    "val_feats = ['score', 'diff_iq_day', 'diff_iq_hour']\n",
    "for fi in cate_feats:\n",
    "    for fj in val_feats:\n",
    "        sub_feats = ['mean', 'median']\n",
    "        for sub in sub_feats:\n",
    "            if fi+'_day_hour_'+fj+'_'+sub not in drop_feat:\n",
    "                drop_feat.append(fi+'_day_hour_'+fj+'_'+sub)\n",
    "\n",
    "last_100_feat = pickle.load(open('./last100col.pkl', 'rb'))\n",
    "print(len(last_100_feat))\n",
    "for f in last_100_feat:\n",
    "    if f not in drop_feat:\n",
    "        drop_feat.append(f)\n",
    "            \n",
    "feature_with_day = [x for x in test.columns if x not in drop_feat]\n",
    "feature_cols = [x for x in test.columns if x not in drop_feat+['day']]\n",
    "print(len(feature_with_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_854 = pickle.load(open('./0.854cols.pkl', 'rb'))\n",
    "not_his_col = ['invite_hour', 'invite_day', 'ffa', 'sex', 'invite_day_count_y_median_x', 'invite_day_count_x_count_y', \n",
    "               'invite_day_count_y_std_y', 'invite_hour_std_y', 'invite_day_count_x_mean_x', 'invite_create', 'invite_day_count_x_mean_y', \n",
    "               'invite_hour_mean_y', 'invite_day_count_x_count_x', 'fa', 'salt_value', 'ffe', 'cross_topic', 'invite_day_count_y_count_x', \n",
    "               'sex_count', 'fd', 'invite_hour_std_x', 'invite_day_count_y', 'invite_day_count_y_median_y', 'create_hour', \n",
    "               'invite_hour_mean_x', 'fb', 'invite_day_count_x_median_y', 'fe', 'ffd', 'invite_day_count_x_std_x', 'ffc',\n",
    "               'invite_day_count_y_mean_x', 'invite_day_count_x_std_y', 'invite_day_count_y_std_x', 'ffb', 'invite_day_count_x_median_x', \n",
    "               'invite_day_count_y_count_y', 'invite_day_count_x', 'create_day', 'fc', 'invite_day_count_y_mean_y']\n",
    "for c1 in col_854:\n",
    "    assert isinstance(c1, str)\n",
    "    if (c1 not in feature_with_day) and (c1 not in not_his_col): \n",
    "        feature_with_day.append(c1)\n",
    "        feature_cols.append(c1)\n",
    "print(len(feature_with_day))\n",
    "print(len(feature_cols))\n",
    "print(len(set(feature_with_day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(feature_with_day, open('./feature_with_day.pkl', 'wb'))\n",
    "# pickle.dump(feature_cols, open('./feature_cols.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_with_day = pickle.load(open('./feature_with_day_in_train.pkl', 'rb'))\n",
    "train_feature_cols = pickle.load(open('./feature_cols_in_train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_feature_with_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(feature_with_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pickle.load(open(f'{newfeature_path}/history_lastweek_test2_sup_a.pkl', 'rb')).reset_index(drop=True)\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pickle.load(open(f'{newfeature_path}/history_ltd6_test2_sup_a.pkl', 'rb')).reset_index(drop=True)\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pickle.load(open(f'{newfeature_path}/history_ltd_test2_sup_a.pkl', 'rb')).reset_index(drop=True)\n",
    "logging.info('t1 shape: %s', t1.shape)\n",
    "test = pd.concat([test, t1], axis=1)\n",
    "logging.info('test shape: %s', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[feature_cols]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "sub.columns = ['qid', 'uid', 'dt']\n",
    "sub_size = len(sub)\n",
    "\n",
    "logging.info(\"sub %s\", sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test)==sub_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = pickle.load(open('./model/lgb_label_3000_round.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub['label'] = model_lgb.predict(X_test[train_feature_cols])\n",
    "# sub.to_csv('./result/1000_add_label_880059.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./result/3000_890439.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
