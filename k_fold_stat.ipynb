{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 08:01:25,936] INFO in <ipython-input-6-7433c7333c71>: invite (9489162, 4)\n",
      "[2019-12-17 08:01:27,095] INFO in <ipython-input-6-7433c7333c71>: test2 (1141718, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "logging.info(\"invite %s\", train.shape)\n",
    "\n",
    "# test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "# test.columns = ['qid', 'uid', 'dt']\n",
    "# logging.info(\"test %s\", test.shape)\n",
    "\n",
    "test2 = pd.read_csv(f'{base_path}/invite_info_evaluate_2_0926.txt', sep='\\t', header=None)\n",
    "test2.columns = ['qid', 'uid', 'dt']\n",
    "logging.info(\"test2 %s\", test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[0][1:]))\n",
    "\n",
    "\n",
    "def extract_hour(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[1][1:]))\n",
    "\n",
    "train['day'] = extract_day(train['dt'])\n",
    "train['hour'] = extract_hour(train['dt'])\n",
    "\n",
    "# test['day'] = extract_day(test['dt'])\n",
    "# test['hour'] = extract_hour(test['dt'])\n",
    "# del train['dt'], test['dt']\n",
    "\n",
    "test2['day'] = extract_day(test2['dt'])\n",
    "test2['hour'] = extract_hour(test2['dt'])\n",
    "del train['dt'], test2['dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 08:02:32,443] INFO in <ipython-input-8-9063dae39e17>: ques (1829900, 3)\n"
     ]
    }
   ],
   "source": [
    "# 加载问题\n",
    "ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t')\n",
    "ques.columns = ['qid', 'q_dt', 'title_t1', 'title_t2', 'desc_t1', 'desc_t2', 'topic']\n",
    "del ques['title_t1'], ques['title_t2'], ques['desc_t1'], ques['desc_t2']\n",
    "logging.info(\"ques %s\", ques.shape)\n",
    "\n",
    "ques['q_day'] = extract_day(ques['q_dt'])\n",
    "ques['q_hour'] = extract_hour(ques['q_dt'])\n",
    "\n",
    "del ques['q_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 08:04:45,241] INFO in <ipython-input-9-cdaa443c2e7e>: ans (4513735, 18)\n"
     ]
    }
   ],
   "source": [
    "# 加载回答\n",
    "ans = pd.read_csv(f'{base_path}/answer_info_0926.txt', header=None, sep='\\t')\n",
    "ans.columns = ['aid', 'qid', 'uid', 'ans_dt', 'ans_t1', 'ans_t2', 'is_good', 'is_rec', 'is_dest', 'has_img',\n",
    "               'has_video', 'word_count', 'reci_cheer', 'reci_uncheer', 'reci_comment', 'reci_mark', 'reci_tks',\n",
    "               'reci_xxx', 'reci_no_help', 'reci_dis']\n",
    "del ans['ans_t1'], ans['ans_t2']\n",
    "logging.info(\"ans %s\", ans.shape)\n",
    "\n",
    "ans['a_day'] = extract_day(ans['ans_dt'])\n",
    "ans['a_hour'] = extract_hour(ans['ans_dt'])\n",
    "del ans['ans_dt']\n",
    "\n",
    "ans = pd.merge(ans, ques, on='qid', how='left')\n",
    "del ques\n",
    "\n",
    "# 回答距提问的天数\n",
    "ans['diff_qa_days'] = ans['a_day'] - ans['q_day']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4折统计\n",
    "def fold_fn(x):\n",
    "    if 3838<=x<=3846:\n",
    "        return 0\n",
    "    if 3847<=x<=3853:\n",
    "        return 1\n",
    "    if 3854<=x<=3860:\n",
    "        return 2\n",
    "    if 3861<=x<=3867:\n",
    "        return 3\n",
    "    else:\n",
    "        return -1     # 更前的一个月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kfold_train_feature(data_df_, ans_df_): \n",
    "    train_df = data_df_.copy()\n",
    "    ans_df = ans_df_.copy()\n",
    "    logging.info(\"ans_df shape %s\", ans_df.shape)\n",
    "    \n",
    "    train_df['fold'] = train_df['day'].apply(fold_fn)\n",
    "    train_df_copy = train_df.copy()\n",
    "    \n",
    "    # 给 ans 加 fold 信息\n",
    "    ans_df['fold'] = ans_df['a_day'].apply(fold_fn)\n",
    "    logging.info(\"ans_df shape %s\", ans_df.shape)\n",
    "    \n",
    "    extract_q_feat = ['q_inv_kfold_mean', 'q_inv_kfold_sum', 'q_inv_kfold_std', 'q_inv_kfold_count']\n",
    "    extract_u_feat = ['u_inv_kfold_mean', 'u_inv_kfold_sum', 'u_inv_kfold_std', 'u_inv_kfold_count']\n",
    "    a_feat = ['is_good', 'is_rec', 'is_dest', 'has_img', 'has_video', 'word_count',\n",
    "              'reci_cheer', 'reci_uncheer', 'reci_comment', 'reci_mark', 'reci_tks',\n",
    "              'reci_xxx', 'reci_no_help', 'reci_dis', 'diff_qa_days']\n",
    "    \n",
    "    extract_a_feat = ['q_ans_kfold_count', 'u_ans_kfold_count']\n",
    "    for col in a_feat:\n",
    "        extract_a_feat += [f'q_{col}_sum', f'q_{col}_max', f'q_{col}_mean', \n",
    "                           f'u_{col}_sum', f'u_{col}_max', f'u_{col}_mean']\n",
    "        \n",
    "    extract_feat = extract_q_feat + extract_u_feat + extract_a_feat\n",
    "    \n",
    "    for feat in extract_feat:\n",
    "        train_df[feat] = -10000\n",
    "        \n",
    "    for fold_ in range(4):\n",
    "        logging.info(\"fold %s\", fold_)\n",
    "        \n",
    "        log_trn = train_df_copy.loc[train_df_copy['fold'] != fold_]    # 提这些记录里的信息\n",
    "        logging.info(\"log_trn shape %s\", log_trn.shape)\n",
    "        val_df = train_df_copy.loc[train_df_copy['fold'] == fold_]\n",
    "        logging.info(\"val_df shape %s\", val_df.shape)\n",
    "        log_ans = ans_df.loc[ans_df['fold'] != fold_]  # 排除掉当前 fold 的 ans\n",
    "        logging.info(\"log_ans shape %s\", log_ans.shape)\n",
    "        \n",
    "        # ques\n",
    "        logging.info(\"question info\")\n",
    "        t1 = log_trn.groupby('qid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "        t1.columns = ['qid'] + extract_q_feat   # 回答率,回答次数,标准差,邀请次数\n",
    "        train_df.loc[train_df['fold']==fold_, extract_q_feat] = pd.merge(val_df, t1, on='qid', \n",
    "                                                                         how='left')[extract_q_feat].values\n",
    "\n",
    "        # user\n",
    "        logging.info(\"user info\")\n",
    "        t1 = log_trn.groupby('uid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "        t1.columns = ['uid'] + extract_u_feat\n",
    "        train_df.loc[train_df['fold']==fold_, extract_u_feat] = pd.merge(val_df, t1, on='uid', \n",
    "                                                                         how='left')[extract_u_feat].values\n",
    "        \n",
    "        # ans\n",
    "        ans_q_group = log_ans.groupby('qid')\n",
    "        ans_u_group = log_ans.groupby('uid')\n",
    "        \n",
    "        logging.info(\"ans: q_ans_kfold_count\")\n",
    "        t1 = ans_q_group['aid'].count().reset_index()\n",
    "        t1.columns = ['qid', 'q_ans_kfold_count']          # 某问题在 answer_info 中的回答次数\n",
    "        train_df.loc[train_df['fold']==fold_, ['q_ans_kfold_count']] = pd.merge(val_df, t1, on='qid', \n",
    "                                                                                how='left')['q_ans_kfold_count'].values\n",
    "        \n",
    "        logging.info(\"ans: u_ans_kfold_count\")\n",
    "        t1 = ans_u_group['aid'].count().reset_index()\n",
    "        t1.columns = ['uid', 'u_ans_kfold_count']          # 某用户在 answer_info 中的回答次数\n",
    "        train_df.loc[train_df['fold']==fold_, ['u_ans_kfold_count']] = pd.merge(val_df, t1, on='uid', \n",
    "                                                                                how='left')['u_ans_kfold_count'].values\n",
    "        \n",
    "        for col in a_feat:\n",
    "            logging.info(\"ans: %s sum max mean\", col)\n",
    "            \n",
    "            t1 = ans_q_group[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "            f_name = [f'q_{col}_sum', f'q_{col}_max', f'q_{col}_mean']\n",
    "            t1.columns = ['qid'] + f_name\n",
    "            train_df.loc[train_df['fold']==fold_, f_name] = pd.merge(val_df, t1, on='qid', \n",
    "                                                                     how='left')[f_name].values\n",
    "            \n",
    "            t1 = ans_u_group[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "            f_name = [f'u_{col}_sum', f'u_{col}_max', f'u_{col}_mean']\n",
    "            t1.columns = ['uid'] + f_name\n",
    "            train_df.loc[train_df['fold']==fold_, f_name] = pd.merge(val_df, t1, on='uid', \n",
    "                                                                     how='left')[f_name].values\n",
    "            \n",
    "    for feat in extract_feat:\n",
    "        assert len(train_df[train_df[feat]==-10000]) == 0\n",
    "    del train_df['fold']\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kfold = extract_kfold_train_feature(train, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kfold_test_feature(test_df_, data_df_, ans_df_): \n",
    "    train_df = data_df_.copy()\n",
    "    logging.info(\"train_df shape %s\", train_df.shape)\n",
    "    test_df = test_df_.copy()\n",
    "    logging.info(\"test_df shape %s\", test_df.shape)\n",
    "    ans_df = ans_df_.copy()\n",
    "    logging.info(\"ans_df shape %s\", ans_df.shape)\n",
    "    \n",
    "    extract_q_feat = ['q_inv_kfold_mean', 'q_inv_kfold_sum', 'q_inv_kfold_std', 'q_inv_kfold_count']\n",
    "    extract_u_feat = ['u_inv_kfold_mean', 'u_inv_kfold_sum', 'u_inv_kfold_std', 'u_inv_kfold_count']\n",
    "#     a_feat = ['is_good', 'is_rec', 'is_dest', 'has_img', 'has_video', 'word_count',\n",
    "#               'reci_cheer', 'reci_uncheer', 'reci_comment', 'reci_mark', 'reci_tks',\n",
    "#               'reci_xxx', 'reci_no_help', 'reci_dis', 'diff_qa_days']\n",
    "    a_feat = ['diff_qa_days']\n",
    "    \n",
    "    extract_a_feat = ['q_ans_kfold_count', 'u_ans_kfold_count']\n",
    "    for col in a_feat:\n",
    "        extract_a_feat += [f'q_{col}_sum', f'q_{col}_max', f'q_{col}_mean', \n",
    "                           f'u_{col}_sum', f'u_{col}_max', f'u_{col}_mean']\n",
    "        \n",
    "    extract_feat = extract_q_feat + extract_u_feat + extract_a_feat\n",
    "\n",
    "    # ques\n",
    "    logging.info(\"question info\")\n",
    "    t1 = train_df.groupby('qid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "    t1.columns = ['qid'] + extract_q_feat   # 回答率,回答次数,标准差,邀请次数\n",
    "    test_df = pd.merge(test_df, t1, on='qid', how='left')\n",
    "\n",
    "    # user\n",
    "    logging.info(\"user info\")\n",
    "    t1 = train_df.groupby('uid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "    t1.columns = ['uid'] + extract_u_feat\n",
    "    test_df = pd.merge(test_df, t1, on='uid', how='left')\n",
    "\n",
    "    # ans\n",
    "    ans_q_group = ans_df.groupby('qid')\n",
    "    ans_u_group = ans_df.groupby('uid')\n",
    "    \n",
    "    logging.info(\"ans: q_ans_kfold_count\")\n",
    "    t1 = ans_q_group['aid'].count().reset_index()\n",
    "    t1.columns = ['qid', 'q_ans_kfold_count']          # 某问题在 answer_info 中的回答次数\n",
    "    test_df = pd.merge(test_df, t1, on='qid', how='left')\n",
    "\n",
    "    logging.info(\"ans: u_ans_kfold_count\")\n",
    "    t1 = ans_u_group['aid'].count().reset_index()\n",
    "    t1.columns = ['uid', 'u_ans_kfold_count']          # 某用户在 answer_info 中的回答次数\n",
    "    test_df = pd.merge(test_df, t1, on='uid', how='left')\n",
    "\n",
    "    for col in a_feat:\n",
    "        logging.info(\"ans: %s sum max mean\", col)\n",
    "        \n",
    "        t1 = ans_q_group[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "        f_name = [f'q_{col}_sum', f'q_{col}_max', f'q_{col}_mean']\n",
    "        t1.columns = ['qid'] + f_name\n",
    "        test_df = pd.merge(test_df, t1, on='qid', how='left')\n",
    "\n",
    "        t1 = ans_u_group[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "        f_name = [f'u_{col}_sum', f'u_{col}_max', f'u_{col}_mean']\n",
    "        t1.columns = ['uid'] + f_name\n",
    "        test_df = pd.merge(test_df, t1, on='uid', how='left')\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kfold = extract_kfold_test_feature(test, train[['uid', 'qid', 'day', 'hour', 'label']], ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 08:05:30,220] INFO in <ipython-input-10-28b7c91e998f>: train_df shape (9489162, 5)\n",
      "[2019-12-17 08:05:30,383] INFO in <ipython-input-10-28b7c91e998f>: test_df shape (1141718, 4)\n",
      "[2019-12-17 08:05:33,421] INFO in <ipython-input-10-28b7c91e998f>: ans_df shape (4513735, 23)\n",
      "[2019-12-17 08:05:33,423] INFO in <ipython-input-10-28b7c91e998f>: question info\n",
      "[2019-12-17 08:05:48,712] INFO in <ipython-input-10-28b7c91e998f>: user info\n",
      "[2019-12-17 08:06:08,240] INFO in <ipython-input-10-28b7c91e998f>: ans: q_ans_kfold_count\n",
      "[2019-12-17 08:06:22,613] INFO in <ipython-input-10-28b7c91e998f>: ans: u_ans_kfold_count\n",
      "[2019-12-17 08:06:31,500] INFO in <ipython-input-10-28b7c91e998f>: ans: diff_qa_days sum max mean\n"
     ]
    }
   ],
   "source": [
    "test_kfold_2 = extract_kfold_test_feature(test2, train[['uid', 'qid', 'day', 'hour', 'label']], ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_kfold['uid'], train_kfold['qid'], train_kfold['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_kfold['uid'], test_kfold['qid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "# t = train_kfold.dtypes\n",
    "# for x in t[t == 'int64'].index:\n",
    "#     train_kfold[x] = train_kfold[x].astype('int32')\n",
    "\n",
    "# for x in t[t == 'float64'].index:\n",
    "#     train_kfold[x] = train_kfold[x].astype('float32')\n",
    "\n",
    "# t = test_kfold.dtypes\n",
    "# for x in t[t == 'int64'].index:\n",
    "#     test_kfold[x] = test_kfold[x].astype('int32')\n",
    "\n",
    "# for x in t[t == 'float64'].index:\n",
    "#     test_kfold[x] = test_kfold[x].astype('float32')\n",
    "\n",
    "# 新测试集\n",
    "t = test_kfold_2.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    test_kfold_2[x] = test_kfold_2[x].astype('int32')\n",
    "\n",
    "for x in t[t == 'float64'].index:\n",
    "    test_kfold_2[x] = test_kfold_2[x].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kfold.to_csv('feature/train_kfold_feature.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kfold.to_csv('feature/test_kfold_feature.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kfold_2.to_csv('feature/test2_kfold_feature.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
