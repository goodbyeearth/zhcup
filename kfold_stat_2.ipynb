{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import logging\n",
    "import multiprocessing\n",
    "import traceback\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "feature_path = './feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-10 09:13:00,413] INFO in <ipython-input-4-af34e33feb4f>: invite (9489162, 4)\n",
      "[2019-12-10 09:13:02,602] INFO in <ipython-input-4-af34e33feb4f>: test (1141683, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "logging.info(\"invite %s\", train.shape)\n",
    "\n",
    "test = pd.read_csv(f'{base_path}/invite_info_evaluate_0926.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt']\n",
    "logging.info(\"test %s\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[0][1:]))\n",
    "\n",
    "\n",
    "def extract_hour(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[1][1:]))\n",
    "\n",
    "train['day'] = extract_day(train['dt'])\n",
    "train['week'] = train['day'] % 7\n",
    "train['hour'] = extract_hour(train['dt'])\n",
    "\n",
    "test['day'] = extract_day(test['dt'])\n",
    "test['week'] = test['day'] % 7\n",
    "test['hour'] = extract_hour(test['dt'])\n",
    "\n",
    "del train['dt'], test['dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-10 09:13:39,852] INFO in <ipython-input-6-cbb4bab8955e>: user (1931654, 14)\n"
     ]
    }
   ],
   "source": [
    "# 加载用户\n",
    "user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "user.columns = ['uid', 'gender', 'freq', 'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "                'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5',  'score', 'follow_topic', 'inter_topic']\n",
    "\n",
    "del user['follow_topic'], user['inter_topic']\n",
    "logging.info(\"user %s\", user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge user\n",
    "train = pd.merge(train, user, on='uid', how='left')\n",
    "test = pd.merge(test, user, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-10 09:14:50,655] INFO in <ipython-input-8-3053b1f3a755>: ques (1829900, 2)\n"
     ]
    }
   ],
   "source": [
    "# 加载问题\n",
    "ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t')\n",
    "ques.columns = ['qid', 'q_dt', 'title_t1', 'title_t2', 'desc_t1', 'desc_t2', 'topic']\n",
    "del ques['title_t1'], ques['title_t2'], ques['desc_t1'], ques['desc_t2'], ques['topic']\n",
    "logging.info(\"ques %s\", ques.shape)\n",
    "\n",
    "ques['q_day'] = extract_day(ques['q_dt'])\n",
    "ques['q_hour'] = extract_hour(ques['q_dt'])\n",
    "ques['q_week'] = ques['q_day'] % 7\n",
    "\n",
    "del ques['q_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ques\n",
    "train = pd.merge(train, ques, on='qid', how='left')\n",
    "test = pd.merge(test, ques, on='qid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['diff_iq_day'] = train['day'] - train['q_day']\n",
    "train['diff_iq_hour'] = train['diff_iq_day'] * 24 + (train['hour'] - train['q_hour'])\n",
    "\n",
    "test['diff_iq_day'] = test['day'] - test['q_day']\n",
    "test['diff_iq_hour'] = test['diff_iq_day'] * 24 + (test['hour'] - test['q_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_iq_day_map(x):\n",
    "    if x>=31:\n",
    "        return 31\n",
    "    if x<0:\n",
    "        return 0\n",
    "    return x\n",
    "\n",
    "train['diff_iq_day'] = train['diff_iq_day'].apply(diff_iq_day_map)\n",
    "test['diff_iq_day'] = test['diff_iq_day'].apply(diff_iq_day_map)\n",
    "\n",
    "def diff_iq_hour_map(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    if x>200:\n",
    "        return 40\n",
    "    return x // 5\n",
    "train['diff_iq_hour'] = train['diff_iq_hour'].apply(diff_iq_hour_map)\n",
    "test['diff_iq_day'] = test['diff_iq_day'].apply(diff_iq_day_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_map(x):\n",
    "    if x<=280:\n",
    "        return -1\n",
    "    if x<=300:\n",
    "        return 0\n",
    "    if 300<x<=350:\n",
    "        return 1\n",
    "    if 350<x<=400:\n",
    "        return 2\n",
    "    if 400<x<=500:\n",
    "        return 3\n",
    "    if 500<x<=600:\n",
    "        return 4\n",
    "    if 600<x<=700:\n",
    "        return 5\n",
    "    if 700<x<=800:\n",
    "        return 6\n",
    "    return 7\n",
    "\n",
    "train['score'] = train['score'].apply(score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 invete feature 2: intersection_ft_count, intersection_it_count\n",
    "t1 = pd.read_csv(f'{feature_path}/train_invite_feature_2.txt', sep='\\t', \n",
    "                 usecols=['intersection_ft_count', 'intersection_it_count'])\n",
    "train = pd.concat([train, t1], axis=1)\n",
    "\n",
    "t1 = pd.read_csv(f'{feature_path}/test_invite_feature_2.txt', sep='\\t', \n",
    "                 usecols=['intersection_ft_count', 'intersection_it_count'])\n",
    "test = pd.concat([test, t1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分 intersection_ft_count\n",
    "def to_bin_1(x):\n",
    "    if x>=3:\n",
    "        return 3\n",
    "    return x\n",
    "\n",
    "train['intersection_ft_count'] = train['intersection_ft_count'].apply(to_bin_1)\n",
    "test['intersection_ft_count'] = test['intersection_ft_count'].apply(to_bin_1)\n",
    "\n",
    "# 划分 intersection_it_count\n",
    "def to_bin_2(x):\n",
    "    if x>=4:\n",
    "        return 4\n",
    "    return x\n",
    "\n",
    "train['intersection_it_count'] = train['intersection_it_count'].apply(to_bin_2)\n",
    "test['intersection_it_count'] = test['intersection_it_count'].apply(to_bin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4折统计\n",
    "def fold_fn(x):\n",
    "    if 3838<=x<=3846:\n",
    "        return 0\n",
    "    if 3847<=x<=3853:\n",
    "        return 1\n",
    "    if 3854<=x<=3860:\n",
    "        return 2\n",
    "    if 3861<=x<=3867:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = train['day'].apply(fold_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 一阶\n",
    "def single_train_feat(df_, feat):\n",
    "    df = df_.copy()\n",
    "    extract_feat_1 = [feat+'_kfold_count', feat+'_label_mean', feat+'_label_sum', feat+'_label_std']\n",
    "#     extract_feat_2 = [feat+'_kfold_hour_count', feat+'_label_hour_mean', \n",
    "#                       feat+'_label_hour_sum', feat+'_label_hour_std']\n",
    "#     extract_feat_3 = [feat+'_kfold_week_count', feat+'_label_week_mean', \n",
    "#                       feat+'_label_week_sum', feat+'_label_week_std']\n",
    "#     extract_feat = extract_feat_1 + extract_feat_2 + extract_feat_3\n",
    "    for c in extract_feat:\n",
    "        df[c] = -10000\n",
    "    for i in range(4):\n",
    "        t1 = df[df['fold']!=i].groupby(feat)['label'].agg(['count', 'mean', 'sum', 'std']).reset_index()\n",
    "        t1.loc[t1['count']<5, ['mean', 'std']] = np.nan\n",
    "        t1.columns = [feat] + extract_feat_1\n",
    "        df.loc[df['fold']==i, extract_feat_1] = pd.merge(df.loc[df['fold']==i, feat], t1, on=feat, \n",
    "                                                         how='left')[extract_feat_1].values\n",
    "        # 某小时\n",
    "#         t1 = df[df['fold']!=i].groupby([feat, 'hour'])['label'].agg(['count', \n",
    "#                                                                      'mean', 'sum', 'std']).reset_index()\n",
    "#         t1.loc[t1['count']<5, ['mean', 'std']] = np.nan\n",
    "#         t1.columns = [feat, 'hour'] + extract_feat_2\n",
    "#         df.loc[df['fold']==i, extract_feat_2] = pd.merge(df.loc[df['fold']==i, [feat, 'hour']], \n",
    "#                                                          t1, on=[feat, 'hour'], \n",
    "#                                                          how='left')[extract_feat_2].values\n",
    "#         # 一周的某一天\n",
    "#         t1 = df[df['fold']!=i].groupby([feat, 'week'])['label'].agg(['count', \n",
    "#                                                                      'mean', 'sum', 'std']).reset_index()\n",
    "#         t1.loc[t1['count']<5, ['mean', 'std']] = np.nan\n",
    "#         t1.columns = [feat, 'week'] + extract_feat_3\n",
    "#         df.loc[df['fold']==i, extract_feat_3] = pd.merge(df.loc[df['fold']==i, [feat, 'week']], \n",
    "#                                                          t1, on=[feat, 'week'], \n",
    "#                                                          how='left')[extract_feat_3].values\n",
    "    # 数据压缩\n",
    "    for c in range(0, len(extract_feat), 2):\n",
    "        df[extract_feat[c]] = df[extract_feat[c]].fillna(0).astype('int32')\n",
    "    for c in range(1, len(extract_feat), 2):\n",
    "        df[extract_feat[c]] = df[extract_feat[c]].astype('float32')\n",
    "\n",
    "    return df[extract_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_targets = ['uid', 'qid', 'freq', 'score', \n",
    "                  'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "                  'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5',\n",
    "                  'diff_iq_day', 'diff_iq_hour', \n",
    "                  'intersection_ft_count', 'intersection_it_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = len(single_targets)\n",
    "\n",
    "def kfold_worker_1(df, feat):\n",
    "    try:\n",
    "        t1 = single_train_feat(df, feat)\n",
    "        logging.info('%s, feature shape: %s', feat, t1.shape)\n",
    "        \n",
    "        pickle.dump(t1, open(f'{feature_path}/single_kfold_feat/train_{feat}.pkl', 'wb'))\n",
    "        logging.info('%s feature saved!', feat)\n",
    "        del t1\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(traceback.print_exct())\n",
    "\n",
    "def multi_proc_train(df, feat_list):\n",
    "    pool = multiprocessing.Pool(processes=n_proc)\n",
    "    for f in feat_list:\n",
    "        pool.apply_async(kfold_worker_1, (df, f))\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proc_train(train, single_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 一阶\n",
    "def single_test_feat(df, feat):\n",
    "    extract_feat_1 = [feat+'_kfold_count', feat+'_label_mean', feat+'_label_sum', feat+'_label_std']\n",
    "    extract_feat_2 = [feat+'_kfold_hour_count', feat+'_label_hour_mean', \n",
    "                      feat+'_label_hour_sum', feat+'_label_hour_std']\n",
    "    extract_feat_3 = [feat+'_kfold_week_count', feat+'_label_week_mean', \n",
    "                      feat+'_label_week_sum', feat+'_label_week_std']\n",
    "    extract_feat = extract_feat_1 + extract_feat_2 + extract_feat_3\n",
    "    \n",
    "    t1 = df.groupby(feat)['label'].agg(['count', 'mean', 'sum', 'std']).reset_index()\n",
    "    t1.loc[t1['count']<5, ['mean', 'std']] = np.nan\n",
    "    t1.columns = [feat] + extract_feat_1\n",
    "    \n",
    "    t2 = df.groupby([feat, 'hour'])['label'].agg(['count', 'mean', 'sum', 'std']).reset_index()\n",
    "    t2.loc[t2['count']<5, ['mean', 'std']] = np.nan\n",
    "    t2.columns = [feat, 'hour'] + extract_feat_2\n",
    "    \n",
    "    t3 = df.groupby([feat, 'week'])['label'].agg(['count', 'mean', 'sum', 'std']).reset_index()\n",
    "    t3.loc[t3['count']<5, ['mean', 'std']] = np.nan\n",
    "    t3.columns = [feat, 'week'] + extract_feat_3\n",
    "    \n",
    "    # 数据压缩\n",
    "    for c in range(0, 4, 2):\n",
    "        t1[extract_feat_1[c]] = ((t1[extract_feat_1[c]])*23/30).astype('int32')\n",
    "        t2[extract_feat_2[c]] = ((t2[extract_feat_2[c]])*23/30).astype('int32')\n",
    "        t3[extract_feat_3[c]] = ((t3[extract_feat_3[c]])*23/30).astype('int32')\n",
    "    for c in range(1, 4, 2):\n",
    "        t1[extract_feat_1[c]] = t1[extract_feat_1[c]].astype('float32')\n",
    "        t2[extract_feat_2[c]] = t2[extract_feat_2[c]].astype('float32')\n",
    "        t3[extract_feat_3[c]] = t3[extract_feat_3[c]].astype('float32')\n",
    "    \n",
    "    return t1, t2, t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = len(single_targets)\n",
    "\n",
    "def kfold_worker_2(train_df, feat):\n",
    "    try:\n",
    "        t1, t2, t3 = single_test_feat(train_df, feat)\n",
    "        logging.info('%s, feature shape: %s', feat, t1.shape)\n",
    "        \n",
    "        pickle.dump(t1, open(f'{feature_path}/single_kfold_feat/test_{feat}_t1.pkl', 'wb'))\n",
    "        pickle.dump(t2, open(f'{feature_path}/single_kfold_feat/test_{feat}_t2.pkl', 'wb'))\n",
    "        pickle.dump(t3, open(f'{feature_path}/single_kfold_feat/test_{feat}_t3.pkl', 'wb'))\n",
    "        logging.info('%s feature saved!', feat)\n",
    "        del t1, t2, t3\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(traceback.print_exct())\n",
    "\n",
    "def multi_proc_test(train_df, feat_list):\n",
    "    pool = multiprocessing.Pool(processes=n_proc)\n",
    "    for f in feat_list:\n",
    "        pool.apply_async(kfold_worker_2, (train_df, f))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proc_test(train, single_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_merge(test_df, feat_df_list, feat):\n",
    "    extract_feat_1 = [feat+'_kfold_count', feat+'_label_mean', feat+'_label_sum', feat+'_label_std']\n",
    "    extract_feat_2 = [feat+'_kfold_hour_count', feat+'_label_hour_mean', \n",
    "                      feat+'_label_hour_sum', feat+'_label_hour_std']\n",
    "    extract_feat_3 = [feat+'_kfold_week_count', feat+'_label_week_mean', \n",
    "                      feat+'_label_week_sum', feat+'_label_week_std']\n",
    "    extract_feat = extract_feat_1 + extract_feat_2 + extract_feat_3\n",
    "    t1 = pd.merge(test, feat_df_list[0], on=[feat], how='left')\n",
    "    t1 = pd.merge(t1, feat_df_list[1], on=[feat, 'hour'], how='left')\n",
    "    t1 = pd.merge(t1, feat_df_list[2], on=[feat, 'week'], how='left')\n",
    "    for i in range(0, len(extract_feat), 2):\n",
    "        t1[extract_feat[i]] = t1[extract_feat[i]].fillna(0).astype('int32')\n",
    "    for i in range(1, len(extract_feat), 2):\n",
    "        t1[extract_feat[i]] = t1[extract_feat[i]].astype('float32')\n",
    "\n",
    "    return t1[extract_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_worker(test_df, feat):\n",
    "    l1 = []\n",
    "    l1.append(pickle.load(open(f'{feature_path}/single_kfold_feat/test_{feat}_t1.pkl', 'rb')))\n",
    "    l1.append(pickle.load(open(f'{feature_path}/single_kfold_feat/test_{feat}_t2.pkl', 'rb')))\n",
    "    l1.append(pickle.load(open(f'{feature_path}/single_kfold_feat/test_{feat}_t3.pkl', 'rb')))\n",
    "    t1 = my_merge(test_df, l1, feat)\n",
    "    logging.info('merged %s feature, shape: %s', feat, t1.shape)\n",
    "    pickle.dump(t1, open(f'{feature_path}/single_kfold_feat/test_{feat}_merged.pkl', 'wb'))\n",
    "\n",
    "def multi_proc_merge(test_df, feat_list):\n",
    "    pool = multiprocessing.Pool(processes=n_proc)\n",
    "    for f in feat_list:\n",
    "        pool.apply_async(merge_worker, (test_df, f))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_targets = ['uid', 'qid', 'freq', 'score', \n",
    "#                   'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "#                   'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5',\n",
    "#                   'diff_iq_day', 'diff_iq_hour', \n",
    "#                   'intersection_ft_count', 'intersection_it_count']\n",
    "multi_proc_merge(test, single_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pickle.load(open('feature/single_kfold_feat/train_uid.pkl', 'rb'))\n",
    "t2 = pickle.load(open('feature/single_kfold_feat/test_uid_merged.pkl', 'rb'))\n",
    "t3 = ['qid', 'freq', 'score', 'uf_b1', 'uf_b2','uf_b3', 'uf_b4', 'uf_b5', \n",
    "      'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', 'diff_iq_day', 'diff_iq_hour', \n",
    "      'intersection_ft_count', 'intersection_it_count']\n",
    "for f in t3:\n",
    "    logging.info('adding kfold label feature, at: %s', f)\n",
    "    \n",
    "    tt1 = pickle.load(open(f'{feature_path}/single_kfold_feat/train_{f}.pkl', 'rb'))\n",
    "    t1 = pd.concat([t1, tt1], axis=1)\n",
    "    logging.info('train shape: %s', t1.shape)\n",
    "    \n",
    "    tt1 = pickle.load(open(f'{feature_path}/single_kfold_feat/test_{f}_merged.pkl', 'rb'))\n",
    "    t2 = pd.concat([t2, tt1], axis=1)\n",
    "    logging.info('test shape: %s', t2.shape)\n",
    "\n",
    "pickle.dump(t1, open(f'{feature_path}/train_kfold_label_feature.pkl', 'wb'))\n",
    "pickle.dump(t2, open(f'{feature_path}/test_kfold_label_feature.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_feat = [['uf_b1', 'uf_b2'], ['uf_b2', 'uf_b3'],]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
